{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain_community.document_loaders.text import TextLoader\n",
    "from langchain_community.document_loaders import (\n",
    "    WebBaseLoader, \n",
    "    PyPDFLoader, \n",
    "    Docx2txtLoader,\n",
    ")\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ[\"OPENAI_API_KEY\"] = \"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\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\RAG_Travel_Assistance\\rag_travel_assistance\\docs\\GBERTPaper.pdf\n",
      "D:\\RAG_Travel_Assistance\\rag_travel_assistance\\docs\\GottbertPaper.pdf\n",
      "\n",
      "Loading content from URL: https://www.seatguru.com/airlines/Lufthansa/baggage.php\n",
      "\n",
      "Content loaded from URL:\n",
      "--------------------------------------------------\n",
      "\n",
      "Document 1:\n",
      "Page Content: \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "Lufthansa: Baggage Fees and Policy - SeatGuru\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Seat Maps\n",
      "Airlines\n",
      "Cheap Flights\n",
      "\n",
      "Comparison Charts\n",
      "\n",
      "\n",
      "\n",
      "Short-haul Economy Class\n",
      "Short-haul First/Business Class\n",
      "Long-haul Economy Class\n",
      "Premium Economy Class\n",
      "Long-haul Business Class\n",
      "Long-haul First Class\n",
      "\n",
      "\n",
      "\n",
      "Rental Cars\n",
      "Guru Tips\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                    Love travel? Sign up for our free newsletter and get the latest news, insights, and money-saving tips.\n",
      "\t\t    \n",
      "\n",
      "\n",
      "\t\t\t    By proceedin...\n",
      "Metadata: {'source': 'https://www.seatguru.com/airlines/Lufthansa/baggage.php', 'title': 'Lufthansa: Baggage Fees and Policy - SeatGuru', 'description': ' Before your next Lufthansa flight, be sure to visit our baggage guide to answer some of the most commonly asked questions.', 'language': 'No language found.'}\n",
      "--------------------------------------------------\n",
      "\n",
      "Total number of documents loaded from URL: 1\n",
      "\n",
      "Total number of all documents loaded: 16\n"
     ]
    }
   ],
   "source": [
    "# Load docs\n",
    "\n",
    "doc_paths = [\n",
    "    \"D:/RAG_Travel_Assistance/rag_travel_assistance/docs/GBERTPaper.pdf\",\n",
    "    \"D:/RAG_Travel_Assistance/rag_travel_assistance/docs/GottbertPaper.pdf\",\n",
    "]\n",
    "\n",
    "docs = [] \n",
    "for doc_file in doc_paths:\n",
    "    file_path = Path(doc_file)\n",
    "    print(file_path)\n",
    "\n",
    "    try:\n",
    "        if doc_file.endswith(\".pdf\"):\n",
    "            loader = PyPDFLoader(file_path)\n",
    "        elif doc_file.endswith(\".docx\"):\n",
    "            loader = Docx2txtLoader(file_path)\n",
    "        elif doc_file.endswith(\".txt\") or doc_file.name.endswith(\".md\"):\n",
    "            loader = TextLoader(file_path)\n",
    "        else:\n",
    "            print(f\"Document type {doc_file.type} not supported.\")\n",
    "            continue\n",
    "\n",
    "        docs.extend(loader.load())\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading document {doc_file.name}: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "url = \"https://www.seatguru.com/airlines/Lufthansa/baggage.php\"\n",
    "try:\n",
    "    print(f\"\\nLoading content from URL: {url}\")\n",
    "    loader = WebBaseLoader(url)\n",
    "    url_docs = loader.load()\n",
    "    docs.extend(url_docs)\n",
    "    \n",
    "    # Print the content from URL\n",
    "    print(\"\\nContent loaded from URL:\")\n",
    "    print(\"-\" * 50)\n",
    "    for i, doc in enumerate(url_docs):\n",
    "        print(f\"\\nDocument {i+1}:\")\n",
    "        print(f\"Page Content: {doc.page_content[:500]}...\")  # Print first 500 characters\n",
    "        print(f\"Metadata: {doc.metadata}\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"\\nTotal number of documents loaded from URL: {len(url_docs)}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading document from {url}: {e}\")\n",
    "\n",
    "# Print summary\n",
    "print(f\"\\nTotal number of all documents loaded: {len(docs)}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'producer': 'pdfTeX-1.40.20', 'creator': 'TeX', 'creationdate': '2020-11-03T16:20:53+00:00', 'author': 'Branden Chan ; Stefan Schweter ; Timo Möller', 'moddate': '2020-11-03T16:20:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1', 'subject': 'COLING2020 2020', 'title': \"German's Next Language Model\", 'trapped': '/False', 'source': 'D:\\\\RAG_Travel_Assistance\\\\rag_travel_assistance\\\\docs\\\\GBERTPaper.pdf', 'total_pages': 9, 'page': 0, 'page_label': '1'}, page_content='Proceedings of the 28th International Conference on Computational Linguistics, pages 6788–6796\\nBarcelona, Spain (Online), December 8-13, 2020\\n6788\\nGerman’s Next Language Model\\nBranden Chan∗†, Stefan Schweter∗‡, Timo M¨oller†\\n†deepset\\n{branden.chan, timo.moeller}@deepset.ai\\n‡Bayerische Staatsbibliothek M¨unchen\\nDigital Library/Munich Digitization Center\\nstefan.schweter@bsb-muenchen.de\\nAbstract\\nIn this work we present the experiments which lead to the creation of our BERT and ELECTRA\\nbased German language models, GBERT and GELECTRA. By varying the input training data,\\nmodel size, and the presence of Whole Word Masking (WWM) we were able to attain SoTA\\nperformance across a set of document classiﬁcation and named entity recognition (NER) tasks\\nfor both models of base and large size. We adopt an evaluation driven approach in training these\\nmodels and our results indicate that both adding more data and utilizing WWM improve model\\nperformance. By benchmarking against existing German models, we show that these models\\nare the best German models to date. Our trained models will be made publicly available to the\\nresearch community.\\n1 Introduction\\nDeep transformer based language models have shown state-of-the-art results for various Natural Lan-\\nguage Processing tasks like text classiﬁcation, NER and question answering (Devlin et al., 2019). They\\nare pretrained, ﬁrst by feeding in large unlabeled text corpora before being ﬁne-tuned on the down-\\nstream task. In this work we present a set of German BERT and ELECTRA models, the best of which,\\nGELECTRALarge, signiﬁcantly improves upon state of the art performance on the GermEval18 hate\\nspeech detection task by about +4% / +2.5% for the coarse and ﬁne variants of the task respectively.\\nThis model also reaches SoTA on the GermEval14 NER task, outperforming the previous best by over\\n+4%. While performant, such models are prohibitively large for many and so we also present a new\\nGBERT model which matches deepset BERT, the previous best German BERT, in size but outperforms\\nit by +2.23% F1 averaged over three tasks.\\nIn the process of pretraining the language models, we also a) quantify the effect of increasing the\\ntraining data by an order of magnitude and b) verify that whole word masking has a positive effect on\\nBERT models.\\nBecause of the computational expense of training large language models from scratch, we adopt a\\ndownstream-oriented evaluation approach to ensure that we get the best performance from a limited\\nnumber of runs. This involves regularly checkpointing the model over the course of pretraining, evalu-\\nating these on a set of classiﬁcation and NER tasks and selecting as ﬁnal the checkpoint which shows\\nthe best performance. This stands in contrast to approaches where the ﬁnal model is simply saved after a\\nﬁxed number of steps. Our method is also an important tool in diagnosing pretraining and we hope that\\nit will be of use to other teams looking to train effective language models on a budget.\\n2 Related work\\nModern language model architectures are trained to build word representations that take into consid-\\neration the context around a given word. First versions such as ELMo (Peters et al., 2018), ULMFiT\\nThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details:\\nhttp://creativecommons.org/licenses/by/4.0/.\\n∗Equal contribution.'), Document(metadata={'producer': 'pdfTeX-1.40.20', 'creator': 'TeX', 'creationdate': '2020-11-03T16:20:53+00:00', 'author': 'Branden Chan ; Stefan Schweter ; Timo Möller', 'moddate': '2020-11-03T16:20:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1', 'subject': 'COLING2020 2020', 'title': \"German's Next Language Model\", 'trapped': '/False', 'source': 'D:\\\\RAG_Travel_Assistance\\\\rag_travel_assistance\\\\docs\\\\GBERTPaper.pdf', 'total_pages': 9, 'page': 1, 'page_label': '2'}, page_content='6789\\n(Howard and Ruder, 2018) and F LAIR (Akbik et al., 2018) are LSTM based and these were able to\\nset new performance benchmarks on downstream tasks like text classiﬁcation, PoS tagging and NER.\\nMore recent approaches use Transformer-based (Vaswani et al., 2017) architectures and examples in-\\nclude GPT-2 (Radford et al., 2019), BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), ALBERT\\n(Lan et al., 2020) and ELECTRA (Clark et al., 2020).\\nIn this work we focus on BERT and ELECTRA models. BERT uses a masked language modeling\\n(MLM) strategy to corrupt an input sentence by replacing some tokens with a [MASK] symbol. The\\nmodel is then trained to re-construct the original token. However, this method of training is somewhat\\nrestricted in that the model only learns from the masked out tokens which typically make up about 15%\\nof the input tokens.\\nELECTRA addresses this problem by introducing a new pretraining task called Replaced Token de-\\ntection. Instead of masking out tokens, a subset of the input tokens are substituted by a synthetically\\ngenerated token. The model is then trained to classify whether each input token is original or substituted,\\nthus allowing for gradient updates at every input position. Practically speaking, this is achieved by hav-\\ning a discriminator that performs the replaced token detection and a generator which provides plausible\\ntoken substitutes. These two components are trained jointly and are both Transformer based.\\nThe BERT model received an update when the original authors added Whole Word Masking1 whereby\\nmasking one subword token requires that all other tokens in the word are also masked out. The authors\\nreport that this method improves the training signal by removing the easiest cases and show that it im-\\nproves performance in their tasks.\\nThere is also a line of work that looks into bringing language modeling techniques that were ﬁrst\\ndeveloped on English to other languages. These include but are not limited to monolingual models\\nsuch as CamemBERT (Martin et al., 2020) and FlauBERT (Le et al., 2020) for French, Finnish BERT\\n(Virtanen et al., 2019) and German BERTs by DBMDZ 2 and deepset3. For a more comprehensive list,\\nsee (Nozza et al., 2020).\\nSome models are also capable of supporting multiple languages such as multilingual BERT\\n(mBERTBase) and XLM-RoBERTa (Conneau et al., 2019). Multilingual BERT is a multilingual model\\nfor 104 different languages 4 trained on Wikipedia dumps. The XLM-RoBERTa model is trained on\\n2.5TB of data from a cleaned Common Crawl corpus (Wenzek et al., 2020) for 100 different languages.\\nIt is worth emphasizing here that systems trained on naturally occurring data will learn pre-existing\\ncultural biases around gender (Bolukbasi et al., 2016), race and religion (Speer, 2017). Critical eval-\\nuation of machine learning methods is more important than ever as NLP is gaining broader adoption.\\nResearchers have been advocating for better documentation of decisions made during the construction of\\na dataset (Gebru et al., 2018), explicit statements of a dataset’s “ingredients” (Holland et al., 2018) and\\nrecognition of the dataset characteristics that may lead to exclusion, overgeneralisation and underexpo-\\nsure (Bender and Friedman, 2018). These topics will be addressed in Section 3.1.\\n3 Datasets\\n3.1 Pretraining Data\\nWe have available to us, a range of different German language corpora that we use in different combi-\\nnations for our model pretraining. OSCAR (Ortiz Su ´arez et al., 2019) is a set of monolingual corpora\\nextracted from Common Crawl. The Common Crawl texts are pre-processed (e.g. HTML entities are\\nremoved) and a language classiﬁcation model is used to sort texts by language. We use the unshufﬂed\\nversion of the German OSCAR corpus, resulting in 145GB of text. The Wikipedia dump for German is\\npreprocessed with the WikiExtractor5 script forming a corpus of size 6GB. The OPUS project 6 (Tiede-\\nmann, 2012) has collected texts from various domains such as movie subtitles, parliament speeches and\\n1https://github.com/google-research/bert/commit/0fce551\\n2https://github.com/dbmdz/berts\\n3https://deepset.ai/german-bert\\n4https://github.com/google-research/bert/blob/f39e88/multilingual.md\\n5https://github.com/attardi/wikiextractor\\n6http://opus.nlpl.eu'), Document(metadata={'producer': 'pdfTeX-1.40.20', 'creator': 'TeX', 'creationdate': '2020-11-03T16:20:53+00:00', 'author': 'Branden Chan ; Stefan Schweter ; Timo Möller', 'moddate': '2020-11-03T16:20:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1', 'subject': 'COLING2020 2020', 'title': \"German's Next Language Model\", 'trapped': '/False', 'source': 'D:\\\\RAG_Travel_Assistance\\\\rag_travel_assistance\\\\docs\\\\GBERTPaper.pdf', 'total_pages': 9, 'page': 2, 'page_label': '3'}, page_content='6790\\nDataset Size\\nOSCAR 145\\nOPUS 10\\nWikipedia 6\\nOpenLegalData 2.4\\nTable 1: The size of each dataset in gigabytes.\\nbooks and these comprise a collection of around 10GB. From Open Legal Data7 (Ostendorff et al., 2020)\\nthere is a dataset of about 2.4GB of German court decisions. Table 1 shows an overview over all datasets.\\nAs discussed in Section 2, our pretrained language models will learn pre-existing biases from the\\ntraining datasets. The main portion (89%) of our training data, namely the OSCAR dataset, uses texts\\nscraped from the internet, which is in some respects problematic. First off, this dataset contains a lot of\\nexplicit and indecent material. While we ﬁltered out many of these documents through keyword match-\\ning, we cannot guarantee that this method was successful in every case. Furthermore, many websites\\ncontain unveriﬁed information and any dataset containing this kind of text can lead to a skewed model\\nthat reﬂects commonly found lies and misconceptions. This includes gender, racial and religious biases\\nwhich are found in textual data of all registers and so we advise that anyone using our model to recognise\\nthat it will not always build true and accurate representation of real world concepts. We implore users\\nof the model to seriously consider these issues before deploying it in a production setting, especially\\nin situations where impartiality matter, such as journalism, and institutional decision making like job\\napplications or insurance assessments.\\n3.2 Downstream Data\\n3.2.1 GermEval18\\nFor text classiﬁcation we use GermEval18 (Coarse) and GermEval18 (Fine) which are both hate speech\\nclassiﬁcation tasks (Wiegand et al., 2018). GermEval18 (Coarse) requires a system to classify a tweet\\ninto one of two classes:OFFENSE if the tweet contains some form of offensive language, andOTHER if it\\ndoes not. GermEval18 (Fine) extends the coarse-grained task and contains four classes: OTHER for non-\\noffensive tweets as well as PROFANITY, INSULT and ABUSE which are all subclasses of OFFENSE\\nfrom the coarse variant of the task.\\n3.2.2 GermEval14\\nFor NER, we use the GermEval14 (Benikova et al., 2014) shared task. The data is sampled from German\\nWikipedia and News Corpora and contains over 31,000 sentences and 590,000 tokens. The dataset is\\none of the largest NER datasets for German and features an advanced annotation schema that allows for\\nnested annotations. The four main classes ( PERSON, ORGANISATION, LOCATION and OTHER) each\\nhave part and derivative variants (e.g. LOCpart or PERderiv) resulting in 12 classes in total.\\n4 Training\\n4.1 Method\\nTo train our German BERT and ELECTRA we use the Tensorﬂow training scripts from the ofﬁcial\\nrepositories8. We train models that match the size of the original BERT Base, BERTLarge, ELECTRABase\\nand ELECTRALarge. The hyperparameters used for training can be found in Table 2. The base models\\nwere trained on single Google Cloud TPUs v3 (8 cores) while large models were trained on pods of 16\\nTPUs v3 (128 cores).\\n7http://openlegaldata.io/research/2019/02/19/court-decision-dataset.html\\n8https://github.com/google-research/bert and https://github.com/google-research/\\nelectra'), Document(metadata={'producer': 'pdfTeX-1.40.20', 'creator': 'TeX', 'creationdate': '2020-11-03T16:20:53+00:00', 'author': 'Branden Chan ; Stefan Schweter ; Timo Möller', 'moddate': '2020-11-03T16:20:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1', 'subject': 'COLING2020 2020', 'title': \"German's Next Language Model\", 'trapped': '/False', 'source': 'D:\\\\RAG_Travel_Assistance\\\\rag_travel_assistance\\\\docs\\\\GBERTPaper.pdf', 'total_pages': 9, 'page': 3, 'page_label': '4'}, page_content='6791\\nGBERTBase GBERTLarge GELECTRABase GELECTRALarge\\nmax sequence length 512 512 512 512\\nbatch size 128 2048 256 1024\\nwarmup steps (k) 10 10 10 30\\nlearning rate 1e-04 1e-04 2e-04 2e-4\\ncheckpoint every (k) 100 100 76.6 100\\nmax train steps (k) 4000 1000 766 1000\\nlayers 12 24 12 24\\nhidden states 768 1024 768 1024\\nattention heads 12 16 12 16\\nvocab size (k) 31 31 31 31\\ntrain time (days) 7 11 8 7\\nTable 2: Hyperparameters for language model pretraining.\\n4.2 Models\\nIn total, we trained 7 separate models with different combinations of data and model size as well as\\nWhole Word Masking (WWM) for BERT models. The German DBMDZ BERT Base, is the same size\\nas BERTBase and was trained using the OPUS and Wikipedia corpora. It serves as our baseline model.\\nWe train four BERT variants of it, each referred to as GBERT, each using the same cased vocabulary as\\nDBMDZ BERTBase. These match BERT Base in size unless they have the ”Large” sufﬁx, in which case\\nthey match BERTLarge:\\n•GBERTData - trained on all available data without Whole Word Masking\\n•GBERTWWM - trained on the same data as DBMDZ BERTBase but uses Whole Word Masking\\n•GBERTData + WWM - trained on all available data and uses Whole Word Masking\\n•GBERTLarge - trained on all available data and uses Whole Word Masking\\nWe also trained three ELECTRA variants of DBMDZ BERT Base, each referred to as GELECTRA\\nmodels, which also match the size of the original ELECTRA Base unless they have the ”Large” sufﬁx in\\nwhich case they match ELECTRALarge:\\n•GELECTRA - trained on same data as DBMDZBase BERT\\n•GELECTRAData - trained on all available data\\n•GELECTRALarge - trained on all available data\\nThe best models of each architecture and size are uploaded to the Hugging Face model hub 9 as\\ndeepset/gbert-base, deepset/gbert-large, deepset/gelectra-base and deepset/gelectra-large.\\n5 Evaluation\\nIn our approach, models are evaluated continuously during pretraining. Model checkpoints are saved at\\nregular intervals and converted into PyTorch models using Hugging Face’s Transformers library (Wolf\\net al., 2019). Using the FARM framework 10, we evaluate the performance of each checkpoint on Ger-\\nmEval18 (Coarse) and GermEval18 (Fine) which are both hate speech classiﬁcation tasks (Wiegand et\\nal., 2018). Using Hugging Face’s Transformers we also evaluate on GermEval14 (Benikova et al., 2014)\\nwhich is a NER task.\\n9https://huggingface.co/models\\n10https://github.com/deepset-ai/FARM'), Document(metadata={'producer': 'pdfTeX-1.40.20', 'creator': 'TeX', 'creationdate': '2020-11-03T16:20:53+00:00', 'author': 'Branden Chan ; Stefan Schweter ; Timo Möller', 'moddate': '2020-11-03T16:20:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1', 'subject': 'COLING2020 2020', 'title': \"German's Next Language Model\", 'trapped': '/False', 'source': 'D:\\\\RAG_Travel_Assistance\\\\rag_travel_assistance\\\\docs\\\\GBERTPaper.pdf', 'total_pages': 9, 'page': 4, 'page_label': '5'}, page_content='6792\\nGermEval18 (Coarse) GermEval18 (Fine) GermEval14\\nType Classiﬁcation Classiﬁcation NER\\nTrain Samples 4509 4509 24002\\nDev Samples 501 501 2200\\nTest Samples 3533 3533 5100\\nClasses 2 4 12\\nMax Epochs 5 5 3\\nMax Train Steps 705 705 4500\\nEvaluation Every 50 steps 50 steps 1500 steps\\nLearning Rate 5e-06 5e-06 5e-05\\nBatch Size 32 32 16\\nMax Seq Len 150 150 128\\nMetric F1 (macro) F1 (macro) F1 (micro)\\nTable 3: Details of the downstream tasks and hyperparameters for model ﬁnetuning for all three tasks.\\nIn BERT, the vector corresponding to the [CLS] token serves as a representation of the whole input\\nsequence, while in ELECTRA, all word vectors are combined through a feed forward layer. In both\\ncases, this input sequence representation is passed through a single layer Neural Network in order to\\nperform prediction. In the NER task, each vector corresponding to the ﬁrst token in a word is passed\\nthrough a single layer Neural Network and the resulting prediction is applied to the whole word.\\nEach checkpoint is evaluated 3 times on each document classiﬁcation task since we observed signif-\\nicant variance across different runs. Each of these runs is performed with early stopping and a differ-\\nent seed each time. For NER, the model is evaluated just once without early stopping. The reported\\nperformance is the average of the single best run for GermEval18 (Coarse), GermEval18 (Fine) and\\nGermEval14. Table 3 summarizes the most important details and parameters of each task. For all exper-\\niments, we use an Nvidia V100 GPU to accelerate training. For each model, we choose the checkpoint\\nthat shows the best performance.\\nFor comparison, we also run this evaluation pipeline on the two publicly available German BERT\\nmodels (deepset German BERT Base and DBMDZ German BERT Base) as well as multilingual models\\nsuch as mBERTBase and XLM-RoBERTaLarge.\\n6 Results\\nThe downstream performance graphs in Figure 1 show that the models are capable of learning with most\\nof the gains being made in the ﬁrst phase of training and more incremental gains coming later. The best\\ncheckpoints come at different points for different models as can be seen in Table 4.\\nIn Table 5 are the evaluation results for each model’s best checkpoint for each of the three downstream\\ntasks with comparison to benchmark models and previous SoTA results. For GermEval18, results from\\nthe best-performing systems are reported (Wiegand et al., 2018). For GermEval14 we report the result\\nthat can be achieved using the FLAIR framework (Akbik et al., 2019).\\nSteps (k)\\nGBERTData 3900\\nGBERTWWM 1500\\nGBERTData + WWM 2000\\nGBERTLarge 900\\nGELECTRA 766\\nGELECTRAData 766\\nGELECTRALarge 1000\\nTable 4: Best checkpoint of each trained model.'), Document(metadata={'producer': 'pdfTeX-1.40.20', 'creator': 'TeX', 'creationdate': '2020-11-03T16:20:53+00:00', 'author': 'Branden Chan ; Stefan Schweter ; Timo Möller', 'moddate': '2020-11-03T16:20:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1', 'subject': 'COLING2020 2020', 'title': \"German's Next Language Model\", 'trapped': '/False', 'source': 'D:\\\\RAG_Travel_Assistance\\\\rag_travel_assistance\\\\docs\\\\GBERTPaper.pdf', 'total_pages': 9, 'page': 5, 'page_label': '6'}, page_content='6793\\nFigure 1: The F1 performance of each model averaged over the three downstream tasks over the course\\nof language model pretraining.\\nIn GermEval18 (Coarse), GBERTData + WWM, XLM-RobertaLarge, GBERTLarge and GELECTRALarge\\nall improve upon the previous SoTA. GELECTRALarge does so with the largest margin reaching a score\\nthat is +3.93% better. In GermEval18 (Fine), XLM-Roberta Large beats the previous best by +1.39% and\\nGELECTRALarge sets a new SoTA that is better than the previous by +2.45%. In GermEval14, all 7\\ntrained models exceed the previous SoTA, with GELECTRA Large showing a +4.3% improvement over\\nthe previous best.\\nThese results indicate that adding extra data gives a consistent but modest performance boost to our\\nlanguage models. GBERT Data outperforms DBMDZ BERTBase by +0.25%, GBERTData + WWM outper-\\nforms GBERTWWM by +0.93% and GELECTRA Data outperforms GELECTRA by +1.59%. For the\\nBERT models, Whole Word Masking also shows a consistent positive impact with GBERTWWM outper-\\nforming DBMDZ BERTBase by +1.70% and GBERTData + WWM outperforming GBERTData by +2.38%.\\n7 Discussion\\n7.1 Model Size\\nThe large models that we train show much stronger performance than the base models. GBERTLarge out-\\nperforms GBERTData + WWM by +2.33% averaged F1 and GELECTRALarge outperforms GELECTRAData'), Document(metadata={'producer': 'pdfTeX-1.40.20', 'creator': 'TeX', 'creationdate': '2020-11-03T16:20:53+00:00', 'author': 'Branden Chan ; Stefan Schweter ; Timo Möller', 'moddate': '2020-11-03T16:20:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1', 'subject': 'COLING2020 2020', 'title': \"German's Next Language Model\", 'trapped': '/False', 'source': 'D:\\\\RAG_Travel_Assistance\\\\rag_travel_assistance\\\\docs\\\\GBERTPaper.pdf', 'total_pages': 9, 'page': 6, 'page_label': '7'}, page_content='6794\\nParams GermEval18 (Coarse)GermEval18 (Fine) GermEval14 Averaged F1\\nDBMDZ BERTBase 110m 75.23 47.39 87.90 69.72\\ndeepset BERTBase 110m 74.7 48.8 86.87 70.12\\nmBERTBase 172m 70.00 45.20 87.44 67.55\\nXLM-RobertaLarge 550m 78.38 54.1 87.07 73.18\\nGBERTData 110m 74.51 48.01 87.41 69.97\\nGBERTWWM 110m 76.48 49.99 87.80 71.42\\nGBERTData + WWM 110m 78.17 50.90 87.98 72.35\\nGBERTLarge 335m 80.08 52.48 88.16 73.57\\nGELECTRA 110m 76.02 42.22 86.02 68.09\\nGELECTRAData 110m 76.59 46.28 86.02 69.63\\nGELECTRALarge 335m 80.70 55.16 88.95 74.94\\nPrevious SoTA 76.77 (TU Wien) 52.71 (uhhLT) 84.65 (FLAIR)\\nTable 5: Downstream evaluation results for the best checkpoints of each GBERT and GELECTRA model\\ncompared to a set of benchmark models. For GermEval18 we report scores for the best-performing\\nsystems (Wiegand et al., 2018), and the result reported by F LAIR framework (Akbik et al., 2019) for\\nGermEval14.\\nby +5.31%. It must be noted however, that their differing training regimes mean that the large models\\nare trained on many more tokens than their base counterparts. In future, we would also be interested in\\ntraining larger models with less data in order to better quantify the gains that come from model size and\\nthe gains that come from the extra data.\\n7.2 Training Length\\nFrom the downstream evaluation graphs in Figure 1, it is clear that the models gain most of their perfor-\\nmance after a relatively short amount of training steps. GBERT WWM and GBERTData + WWM both show\\nan upward trend in the second half of model training suggesting they could still beneﬁt from continuing\\ntraining. There is also a clear upward trend over the course of GELECTRA and GELECTRAData’s train-\\ning suggesting these models are undertrained. It should also be noted that none of the models exhibit any\\nclear signs of overﬁtting or performance degradation and may improve with further training.\\n7.3 ELECTRA Efﬁciency\\nOne of the central claims of the ELECTRA paper is that it is capable of learning more efﬁciently\\nthan MLM based Language Models. This is exempliﬁed by the comparison of GBERT Large and\\nGELECTRALarge. By the end of their 1 million steps of training, GELECTRA Large has only seen half\\nthe number of tokens that GBERT Large due to its smaller batch size and yet outperforms it by +1.47%\\naveraged F1.\\n7.4 Instabilities\\nThe dip in performance around 2 million steps for the base sized GBERT models (See Figure 1) happens\\nto coincide with our training regime whereby the model training is stopped, saved and then reloaded at 2\\nmillion steps. While we suspect that these two events are related, it was beyond the scope of this project\\nto investigate the exact reasons.\\n8 Conclusion\\nThe set of German models which we trained vary in terms of training regime and model architecture. We\\nhope that the results that we present here will serve as important data points to other NLP practitioners\\nwho are looking to train language models from scratch but are limited by compute. Our experiments\\nshould give other teams a sense of the batch sizes and training lengths that make for efﬁcient model\\ntraining. On top of this, we also present a set of GELECTRA and GBERT models which, according to\\nour evaluations, set new SoTA performance for both large and base sized models on GermEval18 and\\nGermEval14.'), Document(metadata={'producer': 'pdfTeX-1.40.20', 'creator': 'TeX', 'creationdate': '2020-11-03T16:20:53+00:00', 'author': 'Branden Chan ; Stefan Schweter ; Timo Möller', 'moddate': '2020-11-03T16:20:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1', 'subject': 'COLING2020 2020', 'title': \"German's Next Language Model\", 'trapped': '/False', 'source': 'D:\\\\RAG_Travel_Assistance\\\\rag_travel_assistance\\\\docs\\\\GBERTPaper.pdf', 'total_pages': 9, 'page': 7, 'page_label': '8'}, page_content='6795\\nAcknowledgements\\nWe would like to thank the deepset team, especially Malte Pietsch and Tanay Soni for their regular\\nsparring and their effort maintaining FARM. Thanks to Zak Stone, Jonathan Caton and everyone at the\\nGoogle TensorFlow Research Cloud team for their advice and for providing us with the access to and\\ncredits for the TPU pods that we used for pretraining. We would also like to thank Nikhil Dinesh from the\\nAWS Activate program as well as Nvidia’s Inception program for providing us with the EC2 instances\\nand credits that allowed us to do large scale evaluation of our models. Thanks also to Pedro Javier Ortiz\\nSu´arez and the OSCAR corpus team for giving us access to their dataset. And thanks to Malte Ostendorff,\\nco-founder of Open Justice e.V ., whose team created Open Legal Data.\\nReferences\\nAlan Akbik, Duncan Blythe, and Roland V ollgraf. 2018. Contextual string embeddings for sequence labeling. In\\nCOLING 2018, 27th International Conference on Computational Linguistics, pages 1638–1649.\\nAlan Akbik, Tanja Bergmann, Duncan Blythe, Kashif Rasul, Stefan Schweter, and Roland V ollgraf. 2019. FLAIR:\\nAn easy-to-use framework for state-of-the-art NLP. In Proceedings of the 2019 Conference of the North Amer-\\nican Chapter of the Association for Computational Linguistics (Demonstrations), pages 54–59, Minneapolis,\\nMinnesota, June. Association for Computational Linguistics.\\nEmily M. Bender and Batya Friedman. 2018. Data statements for natural language processing: Toward mitigating\\nsystem bias and enabling better science. Transactions of the Association for Computational Linguistics, 6:587–\\n604.\\nDarina Benikova, Chris Biemann, Max Kisselew, and Sebastian Pad´o. 2014. Germeval 2014 named entity recog-\\nnition: Companion paper. Proceedings of the KONVENS GermEval Shared Task on Named Entity Recognition,\\nHildesheim, Germany, pages 104–112.\\nTolga Bolukbasi, Kai-Wei Chang, James Y Zou, Venkatesh Saligrama, and Adam T Kalai. 2016. Man is to\\ncomputer programmer as woman is to homemaker? debiasing word embeddings. In D. D. Lee, M. Sugiyama,\\nU. V . Luxburg, I. Guyon, and R. Garnett, editors,Advances in Neural Information Processing Systems 29, pages\\n4349–4357. Curran Associates, Inc.\\nKevin Clark, Minh-Thang Luong, Quoc V . Le, and Christopher D. Manning. 2020. Electra: Pre-training text\\nencoders as discriminators rather than generators. In International Conference on Learning Representations.\\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco\\nGuzm´an, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Unsupervised cross-\\nlingual representation learning at scale.\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirec-\\ntional transformers for language understanding. In Proceedings of the 2019 Conference of the North American\\nChapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and\\nShort Papers), pages 4171–4186, Minneapolis, Minnesota, June. Association for Computational Linguistics.\\nTimnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, III Daum ´e,\\nHal, and Kate Crawford. 2018. Datasheets for Datasets. arXiv e-prints, page arXiv:1803.09010, March.\\nSarah Holland, Ahmed Hosny, Sarah Newman, Joshua Joseph, and Kasia Chmielinski. 2018. The Dataset Nu-\\ntrition Label: A Framework To Drive Higher Data Quality Standards. arXiv e-prints, page arXiv:1805.03677,\\nMay.\\nJeremy Howard and Sebastian Ruder. 2018. Universal language model ﬁne-tuning for text classiﬁcation. In\\nProceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long\\nPapers), pages 328–339, Melbourne, Australia, July. Association for Computational Linguistics.\\nZhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. 2020.\\nAlbert: A lite bert for self-supervised learning of language representations. In International Conference on\\nLearning Representations.\\nHang Le, Lo¨ıc Vial, Jibril Frej, Vincent Segonne, Maximin Coavoux, Benjamin Lecouteux, Alexandre Allauzen,\\nBenoit Crabb ´e, Laurent Besacier, and Didier Schwab. 2020. FlauBERT: Unsupervised language model pre-\\ntraining for French. In Proceedings of The 12th Language Resources and Evaluation Conference, pages 2479–\\n2490, Marseille, France, May. European Language Resources Association.'), Document(metadata={'producer': 'pdfTeX-1.40.20', 'creator': 'TeX', 'creationdate': '2020-11-03T16:20:53+00:00', 'author': 'Branden Chan ; Stefan Schweter ; Timo Möller', 'moddate': '2020-11-03T16:20:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1', 'subject': 'COLING2020 2020', 'title': \"German's Next Language Model\", 'trapped': '/False', 'source': 'D:\\\\RAG_Travel_Assistance\\\\rag_travel_assistance\\\\docs\\\\GBERTPaper.pdf', 'total_pages': 9, 'page': 8, 'page_label': '9'}, page_content='6796\\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke\\nZettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv\\npreprint arXiv:1907.11692.\\nLouis Martin, Benjamin Muller, Pedro Javier Ortiz Su´arez, Yoann Dupont, Laurent Romary, ´Eric Villemonte de la\\nClergerie, Djam´e Seddah, and Benoˆıt Sagot. 2020. Camembert: a tasty french language model. In Proceedings\\nof the 58th Annual Meeting of the Association for Computational Linguistics.\\nDebora Nozza, Federico Bianchi, and Dirk Hovy. 2020. What the [MASK]? Making Sense of Language-Speciﬁc\\nBERT Models. arXiv e-prints, page arXiv:2003.02912, March.\\nPedro Javier Ortiz Su´arez, Benoˆıt Sagot, and Laurent Romary. 2019. Asynchronous Pipeline for Processing Huge\\nCorpora on Medium to Low Resource Infrastructures. In Piotr Ba ´nski, Adrien Barbaresi, Hanno Biber, Evelyn\\nBreiteneder, Simon Clematide, Marc Kupietz, Harald L¨ungen, and Caroline Iliadi, editors, 7th Workshop on the\\nChallenges in the Management of Large Corpora (CMLC-7), Cardiff, United Kingdom, July. Leibniz-Institut\\nf¨ur Deutsche Sprache.\\nMalte Ostendorff, Till Blume, and Saskia Ostendorff. 2020. Towards an open platform for legal information. In\\nProceedings of the ACM/IEEE Joint Conference on Digital Libraries in 2020, JCDL ’20, page 385–388, New\\nYork, NY , USA. Association for Computing Machinery.\\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettle-\\nmoyer. 2018. Deep contextualized word representations. In Proceedings of the 2018 Conference of the North\\nAmerican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1\\n(Long Papers), pages 2227–2237, New Orleans, Louisiana, June. Association for Computational Linguistics.\\nAlec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models\\nare unsupervised multitask learners.\\nRobyn Speer. 2017. Conceptnet numberbatch 17.04: better, less-stereotyped word vectors.\\nJ¨org Tiedemann. 2012. Parallel data, tools and interfaces in opus. In Nicoletta Calzolari (Conference Chair),\\nKhalid Choukri, Thierry Declerck, Mehmet Ugur Dogan, Bente Maegaard, Joseph Mariani, Jan Odijk, and\\nStelios Piperidis, editors, Proceedings of the Eight International Conference on Language Resources and Eval-\\nuation (LREC’12), Istanbul, Turkey, may. European Language Resources Association (ELRA).\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser,\\nand Illia Polosukhin. 2017. Attention is all you need. In I. Guyon, U. V . Luxburg, S. Bengio, H. Wallach,\\nR. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems 30,\\npages 5998–6008. Curran Associates, Inc.\\nAntti Virtanen, Jenna Kanerva, Rami Ilo, Jouni Luoma, Juhani Luotolahti, Tapio Salakoski, Filip Ginter, and\\nSampo Pyysalo. 2019. Multilingual is not enough: BERT for Finnish. arXiv e-prints, page arXiv:1912.07076,\\nDecember.\\nGuillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzm ´an, Armand\\nJoulin, and Edouard Grave. 2020. CCNet: Extracting high quality monolingual datasets from web crawl\\ndata. In Proceedings of The 12th Language Resources and Evaluation Conference, pages 4003–4012, Mar-\\nseille, France, May. European Language Resources Association.\\nMichael Wiegand, Melanie Siegel, and Josef Ruppenhofer. 2018. Overview of the germeval 2018 shared task on\\nthe identiﬁcation of offensive language. In Proceedings of GermEval 2018, 14th Conference on Natural Lan-\\nguage Processing (KONVENS 2018), Vienna, Austria – September 21, 2018, pages 1 – 10. Austrian Academy\\nof Sciences, Vienna, Austria.\\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac,\\nTim Rault, R’emi Louf, Morgan Funtowicz, and Jamie Brew. 2019. Huggingface’s transformers: State-of-the-\\nart natural language processing. ArXiv, abs/1910.03771.'), Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2020-12-04T02:10:31+00:00', 'author': '', 'keywords': '', 'moddate': '2020-12-04T02:10:31+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'D:\\\\RAG_Travel_Assistance\\\\rag_travel_assistance\\\\docs\\\\GottbertPaper.pdf', 'total_pages': 6, 'page': 0, 'page_label': '1'}, page_content='GottBERT: a pure German Language Model\\nRaphael Scheible1, Fabian Thomczyk1, Patric Tippmann1, Victor Jaravine1 and Martin Boeker1\\n{raphael.scheible,thomczyk,tippmann,victor.zharavin,martin.boeker}\\n@imbi.uni-freiburg.de\\n1 Institute of Medical Biometry and Statistics, Medical Center, Faculty of Medicine, University of Freiburg\\nAbstract\\nLately, pre-trained language models advanced\\nthe ﬁeld of natural language processing (NLP).\\nThe introduction of Bidirectional Encoders for\\nTransformers (BERT) and its optimized ver-\\nsion RoBERTa have had signiﬁcant impact and\\nincreased the relevance of pre-trained models.\\nFirst, research in this ﬁeld mainly started on\\nEnglish data followed by models trained with\\nmultilingual text corpora. However, current re-\\nsearch shows that multilingual models are in-\\nferior to monolingual models. Currently, no\\nGerman single language RoBERTa model is\\nyet published, which we introduce in this work\\n(GottBERT). The German portion of the OS-\\nCAR data set was used as text corpus. In\\nan evaluation we compare its performance\\non the two Named Entity Recognition (NER)\\ntasks Conll 2003 and GermEval 2014 as well\\nas on the text classiﬁcation tasks GermEval\\n2018 (ﬁne and coarse) and GNAD with ex-\\nisting German single language BERT models\\nand two multilingual ones. GottBERT was\\npre-trained related to the original RoBERTa\\nmodel using fairseq. All downstream tasks\\nwere trained using hyperparameter presets\\ntaken from the benchmark of German BERT.\\nThe experiments were setup utilizing FARM.\\nPerformance was measured by the F1 score.\\nGottBERT was successfully pre-trained on a\\n256 core TPU pod using the RoBERTa BASE\\narchitecture. Even without extensive hyper-\\nparameter optimization, in all NER and one\\ntext classiﬁcation task, GottBERT already out-\\nperformed all other tested German and multi-\\nlingual models. In order to support the Ger-\\nman NLP ﬁeld, we publish GottBERT under\\nthe AGPLv3 license.\\n1 Introduction\\nThe computation of contextual pre-trained word\\nrepresentation is the current trend of neural net-\\nworks in natural language processing (NLP).\\nThis trend has a long history, starting with non-\\ncontextualized word representations of which the\\nmost prominent are word2vec (Goldberg and Levy,\\n2014) , GloVe (Pennington et al., 2014) and fast-\\nText (Joulin et al., 2016, 2017; Bojanowski et al.,\\n2017; Mikolov et al., 2018), evolving to deep con-\\ntextualized models such as ELmO (Alsentzer et al.,\\n2019) and ﬂair (Akbik et al., 2019). Most recently,\\nthe ﬁeld of NLP experienced remarkable progress,\\nby the use of Transformer (Vaswani et al., 2017)\\nbased approaches. Especially Bidirectional En-\\ncoder Representations from Transformers (BERT)\\n(Devlin et al., 2019) impacted the ﬁeld which sub-\\nsequently was robustly optimized to RoBERTa (Liu\\net al., 2019). These Transformer based approaches\\nuse large scale pre-trained language models. For\\napplication, such models are ﬁne tuned by a super-\\nvised training on the speciﬁc downstream task lead-\\ning to performance improvements in many tasks.\\nOn the other hand, the computation of the language\\nmodel is performed unsupervised. Large text blobs\\nare required for training and strong hardware such\\nas hundreds of GPUs (Martin et al., 2020) or TPUs\\n(You et al., 2020). Initially, most of the research\\ntook place in English followed by multilingual ap-\\nproaches (Conneau et al., 2019). Although, mul-\\ntilingual approaches were trained on large texts\\nof many languages, they were outperformed by\\nsingle language models (de Vries et al., 2019; Mar-\\ntin et al., 2020; Le et al., 2020; Delobelle et al.,\\n2020). Single language models trained with the\\nOpen Super-large Crawled ALMAnaCH coRpus\\n(OSCAR) (Ortiz Su´arez et al., 2020) showed good\\nperformance due to the size and variance of the OS-\\nCAR (Martin et al., 2020; Delobelle et al., 2020).\\nFollowing this ongoing trend, we pre-train the ﬁrst\\nGerman RoBERTa single language model with the\\nGerman portion of the OSCAR - the German OS-\\nCAR text trained BERT (GottBERT). In an evalua-\\ntion we compare its performance on the two named\\narXiv:2012.02110v1  [cs.CL]  3 Dec 2020'), Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2020-12-04T02:10:31+00:00', 'author': '', 'keywords': '', 'moddate': '2020-12-04T02:10:31+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'D:\\\\RAG_Travel_Assistance\\\\rag_travel_assistance\\\\docs\\\\GottbertPaper.pdf', 'total_pages': 6, 'page': 1, 'page_label': '2'}, page_content='entity recognition tasks Conll 2003 and GermEval\\n2014, as well as on the two text classiﬁcation tasks\\nGermEval 2018 and GNAD with existing German\\nsingle language BERT models and two multilingual\\nones.\\n2 Related Work\\nMost recently Transformer based models widely\\nimpacted the ﬁeld of NLP. From neural translation\\n(Ott et al., 2018; Ng et al., 2019) to generative\\nlanguage models as GPT2 (Radford et al., 2019),\\nremarkable performance gains were achieved. With\\nBERT, an approach facilitating pre-trained trans-\\nformer based models was introduced. Fine-tuned\\non downstream tasks, BERT based approaches\\nimproved the performance of several NLP tasks\\n(Devlin et al., 2019; Liu et al., 2019). BERT\\nmodels though, were ﬁrst released as single lan-\\nguage models in English based on 16GB of raw\\ntext and as the multilingual model mBERT based\\non Wikipedia dumps in about 100 languages (De-\\nvlin, 2018). These models were followed by sin-\\ngle language models for several languages: Bertje\\n(de Vries et al., 2019) for Dutch, FinBERT (Virta-\\nnen et al., 2019) for Finish, German BERT 1 and a\\nGerman BERT from the MDZ Digital Library team\\nat the Bavarian State Library to which we refer as\\ndbmz BERT in this paper 2. German BERT was\\ntrained using 12GB of raw text data basing on Ger-\\nman Wikipedia (6GB), the OpenLegalData dump\\n(2.4GB) and news articles (3.6GB). dbmz BERT\\nused as source data a German Wikipedia dump, EU\\nBookshop corpus, Open Subtitles, CommonCrawl,\\nParaCrawl and News Crawl which sums up to a\\ndataset of 16GB. With the release of RoBERTa a\\nnew standard for raw text size was set as it was\\ntrained on 160GB of raw English text. Further,\\nRoBERTa enhances the original BERT approach\\nby removing segment embeddings, next sentence\\nprediction and improved hyperparameters. Ad-\\nditionally, instead of using wordpiece (Schuster\\nand Nakajima, 2012) tokenization, RoBERTa uti-\\nlizes GPT2’s byte pair encoding (BPE) (Radford\\net al., 2019) with the beneﬁt that language-speciﬁc\\ntokenizers are not required. Other than mBERT,\\nthe multilingual XLM-RoBERTa (Conneau et al.,\\n2019) was trained on 2.5TB of ﬁltered Common-\\nCrawl data. CamemBERT is a French RoBERTa\\n1https://deepset.ai/german-bert\\n2https://github.com/dbmdz/berts#\\ngerman-bert\\nmodel that was trained on the OSCAR and uses\\nsentencepiece (Kudo and Richardson, 2018) BPE.\\nFurther, they pre-trained a model with 4GB of the\\nFrench OSCAR portion and another model with\\n4GB of the French Wikipedia. The comparison\\nof these models using downstream tasks shows\\nthat high text variance leads to better results. Um-\\nBERTo3 is an Italian RoBERTa model, similarly\\ndesigned as CamemBERT. RobBERT, the Dutch\\nsingle language RoBERTa, was trained on 39GB\\nof the Dutch portion of the OSCAR and outper-\\nformed Bertje. A more recent version of RobBert\\nshowed the performance gains of language speciﬁc\\nBPE compared to the English based GPT2 BPE\\nin downstream tasks. Most recently FlauBERT\\n(Le et al., 2020) for French was released trained\\non 71GB data. They cleaned a 270GB corpus of\\nmixed sources by ﬁltering out meaningless con-\\ntent and Unicode-normalization. Data was pre-\\ntokenized by moses (Koehn et al., 2007) and en-\\ncoded by fastBPE4 which is an implementation of\\nSennrich et al. (2016). Following the approach of\\nutilizing the OSCAR, we computed the German\\nOSCAR text trained BERT (GottBERT). However,\\nthe drawback of BERT approaches is the computa-\\ntional power requirement. Multiple GPUs or TPUs\\nwere used for pre-training. All listed RoBERTa\\nbased models were computed on GPUs whereas\\nGottBERT is the ﬁrst published RoBERTa model\\npre-trained on TPUs.\\n3 GottBERT\\nGottBERT bases on the robustly optimized BERT\\narchitecture RoBERTa, which was pre-trained with\\nthe German portion of the OSCAR using the fairseq\\nframework (Ott et al., 2019).\\nTraining Data\\nThe GottBERT model is trained on the German\\nportion of the OSCAR, a recently published large\\nmultilingual text corpus extracted from Common\\nCrawl. The German data portion of the OSCAR\\nmeasures 145GB of text containing approximately\\n21.5 billion words in approximately 459 million\\ndocuments (one document per line).\\nPre-processing\\nOriginally, RoBERTa uses GPT2 (Radford et al.,\\n2019) byte pair encoding to segment the input into\\n3https://github.com/\\nmusixmatchresearch/umberto\\n4https://github.com/glample/fastBPE'), Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2020-12-04T02:10:31+00:00', 'author': '', 'keywords': '', 'moddate': '2020-12-04T02:10:31+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'D:\\\\RAG_Travel_Assistance\\\\rag_travel_assistance\\\\docs\\\\GottbertPaper.pdf', 'total_pages': 6, 'page': 2, 'page_label': '3'}, page_content='subword units. Therefore, no pre-tokenization is\\nrequired and thus no language-speciﬁc tokenizer\\nas e.g. moses (Koehn et al., 2007) must be used.\\nIts original vocabulary was computed on English\\ndata. For GottBERT we computed a vocabulary\\nof 52k subword tokens based on 40 GB randomly\\nsampled documents of the German OSCAR portion.\\nCompared to the original GPT2 tokenizer, which\\nwas trained on English data, this leads to a 40%\\nsmaller size of the binary data which are fed into\\nfairseq (Ott et al., 2019). Furthermore, according\\nto Delobelle et al. (2020), it leads to a performance\\nincrease.\\nPre-training\\nUsing fairseq, we pre-trained GottBERT on a 256\\ncore TPU pod using the RoBERTa BASE architec-\\nture. We trained the model in 100k update steps\\nusing a batch size of 8k. A 10k iteration warmup\\nof the learning rate to a peak of 0.0004 was applied,\\nfrom which the learning rate polynomially decayed\\nto zero.\\nDownstream Tasks\\nBased on the pre-trained BERT models, several\\ndownstream tasks were trained. The Framework for\\nAdapting Representation Models (FARM)5 already\\ncomes with pre-conﬁgured experiments for Ger-\\nman language. Originally, these experiments were\\nsetup to benchmark German BERT. We adopted\\nthis set of hyperparameters without additional grid\\nsearch optimization. FARM is based on Hugging\\nFace’s Transformer library (Wolf et al., 2019). As\\nwe trained GottBERT with fairseq, we converted\\nGottBERT into the Hugging Face format.\\nNamed Entity Recognition We evaluated Got-\\ntBERT on two NER tasks. One was the German\\npart of CoNLL 2003 shared task (Tjong Kim Sang\\nand De Meulder, 2003). It contains three main en-\\ntity classes and one for other miscellaneous entities.\\nAs measurement we used the harmonic mean of\\nprecision and recallF1. The second NER task was\\nGermEval 2014 (Benikova et al., 2014). It extends\\nthe CoNLL 2003 shared task by ﬁne-grained labels\\nand embedded markables. Fine-grained labels al-\\nlow the indication of NER subtypes common in\\nGerman, namely derivations and parts: e.g. “Mann”\\n→ “m¨annlich” and “Mann” → “mannhaft”. In\\norder to recognize nested NEs embedded mark-\\nables are required. Speciﬁcally, this was realized\\n5https://github.com/deepset-ai/FARM\\nby annotating main classes as well as two levels of\\nsubclasses. Performance was measured by the use\\nof an adapted F1 evaluation metric Benikova et al.\\n(2014), which considers the equality of labels and\\nspans (text passages) and additionally levels in the\\nclass hierarchy.\\nText Classiﬁcation GermEval task 2018 (Risch\\net al., 2018) is a text classiﬁcation task that contains\\ntwo subtasks of different granularity: the coarse-\\ngrained binary classiﬁcation of German tweets and\\nﬁne-grained classiﬁcation of the same tweets into\\nfour different classes. Based on the One Million\\nPosts Corpus (Schabus et al., 2017) which is in-\\ntended to test performance on German language,\\nthe 10k German News Articles Dataset (10kG-\\nNAD) topic classiﬁcation6 was created. The dataset\\ncontains approximately 10k news articles of an\\nAustrian newspaper which are to be classiﬁed into\\n9 categories. As both classiﬁcation datasets do not\\nprovide a pre-deﬁned validation set, we used 90%\\nof the original training set for training and 10% for\\nvalidation. Data was split randomly. For evaluation\\nwe computed the mean of the F1-scores of each\\nclass/category.\\n4 Results\\nIn order to evaluate the performance, each down-\\nstream task ran 10 times using different seeds. As\\nmeasure, the F1 scores of the experiments based\\non the test set was used. The score is the best of 10\\nruns of the respective experiment of each trained\\nmodel. The best score selection is based on valida-\\ntion set. We compared GottBERT’s performance\\nwith four other models listed in Table 1.\\nNamed Entity Recognition For the two tested\\nNER tasks, CoNLL 2003 and GermEval 2014, Got-\\ntBERT outperformed all others models followed by\\ndbmz BERT (see Table 2). Obviously, the amount\\nand characteristic of data of the German portion\\nof the OSCAR is beneﬁcial. The third place goes\\nto XLM RoBERTa. Interestingly, mBERT outper-\\nforms German BERT, although mBERT’s amount\\nof German data can be assumed to be smaller com-\\npared to German BERT. Notably, mBERT performs\\nan exponentially smoothed weighting of the data in\\norder to balance the variance in data size of all the\\ndifferent languages. Consequently, even if we knew\\nthe data size of the German portion of mBERT, a\\ndirect comparison would not be possible.\\n6https://tblock.github.io/10kGNAD'), Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2020-12-04T02:10:31+00:00', 'author': '', 'keywords': '', 'moddate': '2020-12-04T02:10:31+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'D:\\\\RAG_Travel_Assistance\\\\rag_travel_assistance\\\\docs\\\\GottbertPaper.pdf', 'total_pages': 6, 'page': 3, 'page_label': '4'}, page_content='Model Type #Languages Data Size Data Source\\nGottBERT RoBERTa 1 145GB OSCAR\\ndbmz BERT BERT 1 16GB\\nWikipedia, EU Bookshop corpus7,\\nOpen Subtitles,\\nCommon-,Para-,NewsCrawl\\nmBERTcased BERT 104 unknown Wikipedia\\nGerman BERT BERT 1 12GB news articles, Open Legal Data8,\\nWikipedia\\nXLM RoBERTa RoBERTa 100 2.5TB\\n(66.6GB German) CommonCrawl, Wikipedia\\nTable 1: This table shows the models, we used in our experiments. Additional information about the pre-training\\nand architecture is listed. Unfortunately, for mBERT we did not ﬁnd any estimate about the data size.\\nModel CoNLL 2003 GermEval 2014\\nGottBERT 83.57 86.84\\ndbmz BERT 82.30 85.82\\nmBERTcased 81.20 85.39\\nGerman BERT 81.18 85.03\\nXLM RoBERTa 81.36 85.41\\nTable 2: F1 scores of NER experiments based on the test set. Best score out of 10 runs (selection based on\\nvalidation set).\\nText Classiﬁcation Table 2 shows the F1 scores\\nof the classiﬁcation tasks. In GermEval 2018\\ncoarse and 10kGNAD, dbmz BERT outperforms\\nthe other models followed by German BERT. Got-\\ntBERT only performs best in GermEval 2018 ﬁne.\\nIn 10kGNAD even XLM RoBERTa outperforms\\nGottBERT. In general, the two RoBERTa based\\nmodels seem not to develop their full potential,\\nwhich might be due to sub-optimal hyperparame-\\nters.\\n5 Conclusion\\nIn this work we present the German single lan-\\nguage RoBERTa based model GottBERT which\\nwas computed on 145GB plain text. GottBERT is\\nthe ﬁrst German single language RoBERTa based\\nmodel. Even without extensive hyperparameter op-\\ntimization, in three out of ﬁve downstream tasks,\\nGottBERT already outperformed all other tested\\nmodels. As the authors of German BERT released\\nthe parameters within the FARM framework, we as-\\nsume the parameters for the downstream tasks are\\nprobably not optimal for RoBERTa based models.\\nConsequently, further extensive hyperparameter op-\\ntimization of the downstream tasks might lead to\\nbetter results for GottBERT. Dodge et al. (2020)\\ngive insights into hyperparameter optimization, its\\ncomplexity and effects in relation to BERT. Fur-\\nther, they present a solution to lower costs. Due\\nto the required effort, we currently leave this open\\nas future work and release GottBERT in hugging-\\nface and fairseq format to the community under the\\nAGPLv3 license.\\nAcknowledgments\\nThis work was supported by the German Min-\\nistry for Education and Research (BMBF FKZ\\n01ZZ1801B) and supported with Cloud TPUs from\\nGoogle’s TensorFlow Research Cloud (TFRC). We\\nwould like to thank Ian Graham for constructive\\ncriticism of the manuscript and Louis Martin for\\nthe helping email contact. A special thanks goes\\nto Myle Ott, who implemented the TPU feature\\nin fairseq and intensively supported us to get our\\ncomputation run. Finally, we would like to recog-\\nnizably thank the people behind the scenes who\\nessentially made this work possible: Frank Werner,\\nGeorg Koch, Friedlinde B¨uhler and Jochen Knaus\\nof our internal IT team, Philipp Munz and Chris-\\ntian Wiedemann of Wabion GmbH and last but not\\nleast Nora Limbourg the Google Cloud Customer\\nEngineer assigned to us.'), Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2020-12-04T02:10:31+00:00', 'author': '', 'keywords': '', 'moddate': '2020-12-04T02:10:31+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'D:\\\\RAG_Travel_Assistance\\\\rag_travel_assistance\\\\docs\\\\GottbertPaper.pdf', 'total_pages': 6, 'page': 4, 'page_label': '5'}, page_content='Model GermEval 2018 coarse GermEval 2018 ﬁne 10kGNAD\\nGottBERT 76.39 51.25 89.20\\ndbmz BERT 77.32 50.97 90.86\\nmBERTcased 72.87 44.78 88.72\\nGerman BERT 77.23 49.28 90.66\\nXLM RoBERTa 75.15 45.63 89.30\\nTable 3: F1 scores of text classiﬁcation experiments based on the test set. Best score out of 10 runs (selection\\nbased on validation set).\\nReferences\\nAlan Akbik, Tanja Bergmann, Duncan Blythe, Kashif\\nRasul, Stefan Schweter, and Roland V ollgraf. 2019.\\nFLAIR: An Easy-to-Use Framework for State-of-\\nthe-Art NLP. In Proceedings of the 2019 Confer-\\nence of the North American Chapter of the Asso-\\nciation for Computational Linguistics (Demonstra-\\ntions), pages 54–59, Minneapolis, Minnesota. Asso-\\nciation for Computational Linguistics.\\nEmily Alsentzer, John R. Murphy, Willie Boag, Wei-\\nHung Weng, Di Jin, Tristan Naumann, and Matthew\\nB. A. McDermott. 2019. Publicly Available Clinical\\nBERT Embeddings. arXiv:1904.03323 [cs]. ArXiv:\\n1904.03323.\\nDarina Benikova, Chris Biemann, Max Kisselew, and\\nSebastian Pad ´o. 2014. GermEval 2014 Named En-\\ntity Recognition Shared Task: Companion Paper.\\nProceedings of the KONVENS GermEval Shared\\nTask on Named Entity Recognition, pages 104–112.\\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and\\nTomas Mikolov. 2017. Enriching Word Vectors with\\nSubword Information. Transactions of the Associa-\\ntion for Computational Linguistics, 5:135–146.\\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\\nVishrav Chaudhary, Guillaume Wenzek, Francisco\\nGuzm´an, Edouard Grave, Myle Ott, Luke Zettle-\\nmoyer, and Veselin Stoyanov. 2019. Unsupervised\\ncross-lingual representation learning at scale. CoRR,\\nabs/1911.02116.\\nPieter Delobelle, Thomas Winters, and Bettina Berendt.\\n2020. RobBERT: a Dutch RoBERTa-based Lan-\\nguage Model. arXiv:2001.06286 [cs]. ArXiv:\\n2001.06286.\\nJacob Devlin. 2018. Multilingual BERT Readme Doc-\\nument.\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\\nKristina Toutanova. 2019. BERT: Pre-training of\\nDeep Bidirectional Transformers for Language Un-\\nderstanding. In Proceedings of the 2019 Conference\\nof the North American Chapter of the Association\\nfor Computational Linguistics: Human Language\\nTechnologies, Volume 1 (Long and Short Papers),\\npages 4171–4186, Minneapolis, Minnesota. Associ-\\nation for Computational Linguistics.\\nJesse Dodge, Gabriel Ilharco, Roy Schwartz, Ali\\nFarhadi, Hannaneh Hajishirzi, and Noah Smith.\\n2020. Fine-Tuning Pretrained Language Models:\\nWeight Initializations, Data Orders, and Early Stop-\\nping. arXiv:2002.06305 [cs]. ArXiv: 2002.06305.\\nYoav Goldberg and Omer Levy. 2014. word2vec Ex-\\nplained: deriving Mikolov et al.’s negative-sampling\\nword-embedding method. arXiv:1402.3722 [cs,\\nstat]. ArXiv: 1402.3722.\\nArmand Joulin, Edouard Grave, Piotr Bojanowski,\\nMatthijs Douze, H ´erve J ´egou, and Tomas Mikolov.\\n2016. FastText.zip: Compressing text classiﬁcation\\nmodels. arXiv preprint arXiv:1612.03651.\\nArmand Joulin, Edouard Grave, Piotr Bojanowski, and\\nTomas Mikolov. 2017. Bag of Tricks for Efﬁcient\\nText Classiﬁcation. In Proceedings of the 15th Con-\\nference of the European Chapter of the Association\\nfor Computational Linguistics: Volume 2, Short Pa-\\npers, pages 427–431. Association for Computational\\nLinguistics.\\nPhilipp Koehn, Hieu Hoang, Alexandra Birch, Chris\\nCallison-Burch, Marcello Federico, Nicola Bertoldi,\\nBrooke Cowan, Wade Shen, Christine Moran,\\nRichard Zens, Chris Dyer, Ondˇrej Bojar, Alexandra\\nConstantin, and Evan Herbst. 2007. Moses: Open\\nSource Toolkit for Statistical Machine Translation.\\nIn Proceedings of the 45th Annual Meeting of the\\nAssociation for Computational Linguistics Compan-\\nion Volume Proceedings of the Demo and Poster Ses-\\nsions, pages 177–180, Prague, Czech Republic. As-\\nsociation for Computational Linguistics.\\nTaku Kudo and John Richardson. 2018. SentencePiece:\\nA simple and language independent subword tok-\\nenizer and detokenizer for Neural Text Processing.\\nIn Proceedings of the 2018 Conference on Empirical\\nMethods in Natural Language Processing: System\\nDemonstrations, pages 66–71, Brussels, Belgium.\\nAssociation for Computational Linguistics.\\nHang Le, Lo¨ıc Vial, Jibril Frej, Vincent Segonne, Max-\\nimin Coavoux, Benjamin Lecouteux, Alexandre Al-\\nlauzen, Benoˆıt Crabb´e, Laurent Besacier, and Didier\\nSchwab. 2020. FlauBERT: Unsupervised Language\\nModel Pre-training for French. arXiv:1912.05372\\n[cs]. ArXiv: 1912.05372.\\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,'), Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2020-12-04T02:10:31+00:00', 'author': '', 'keywords': '', 'moddate': '2020-12-04T02:10:31+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'D:\\\\RAG_Travel_Assistance\\\\rag_travel_assistance\\\\docs\\\\GottbertPaper.pdf', 'total_pages': 6, 'page': 5, 'page_label': '6'}, page_content='Luke Zettlemoyer, and Veselin Stoyanov. 2019.\\nRoBERTa: A Robustly Optimized BERT Pretrain-\\ning Approach. arXiv:1907.11692 [cs]. ArXiv:\\n1907.11692.\\nLouis Martin, Benjamin Muller, Pedro Javier Or-\\ntiz Su ´arez, Yoann Dupont, Laurent Romary, ´Eric\\nde la Clergerie, Djam ´e Seddah, and Beno ˆıt Sagot.\\n2020. CamemBERT: a Tasty French Language\\nModel. In Proceedings of the 58th Annual Meet-\\ning of the Association for Computational Linguistics,\\npages 7203–7219, Online. Association for Computa-\\ntional Linguistics.\\nTomas Mikolov, Edouard Grave, Piotr Bojanowski,\\nChristian Puhrsch, and Armand Joulin. 2018. Ad-\\nvances in Pre-Training Distributed Word Representa-\\ntions. In Proceedings of the Eleventh International\\nConference on Language Resources and Evaluation\\n(LREC 2018), Miyazaki, Japan. European Language\\nResources Association (ELRA).\\nNathan Ng, Kyra Yee, Alexei Baevski, Myle Ott,\\nMichael Auli, and Sergey Edunov. 2019. Facebook\\nFAIR’s WMT19 News Translation Task Submission.\\nIn Proceedings of the Fourth Conference on Ma-\\nchine Translation (Volume 2: Shared Task Papers,\\nDay 1), pages 314–319, Florence, Italy. Association\\nfor Computational Linguistics.\\nPedro Javier Ortiz Su´arez, Laurent Romary, and Benoˆıt\\nSagot. 2020. A Monolingual Approach to Contex-\\ntualized Word Embeddings for Mid-Resource Lan-\\nguages. In Proceedings of the 58th Annual Meet-\\ning of the Association for Computational Linguistics,\\npages 1703–1714, Online. Association for Computa-\\ntional Linguistics.\\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela\\nFan, Sam Gross, Nathan Ng, David Grangier, and\\nMichael Auli. 2019. fairseq: A Fast, Extensible\\nToolkit for Sequence Modeling. arXiv:1904.01038\\n[cs]. ArXiv: 1904.01038.\\nMyle Ott, Sergey Edunov, David Grangier, and\\nMichael Auli. 2018. Scaling Neural Machine Trans-\\nlation. arXiv:1806.00187 [cs]. ArXiv: 1806.00187.\\nJeffrey Pennington, Richard Socher, and Christopher\\nManning. 2014. GloVe: Global Vectors for Word\\nRepresentation. In Proceedings of the 2014 Con-\\nference on Empirical Methods in Natural Language\\nProcessing (EMNLP) , pages 1532–1543, Doha,\\nQatar. Association for Computational Linguistics.\\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\\nDario Amodei, and Ilya Sutskever. 2019. Language\\nmodels are unsupervised multitask learners. OpenAI\\nBlog, 1(8):9.\\nJulian Risch, Eva Krebs, Alexander L ¨oser, Alexander\\nRiese, and Ralf Krestel. 2018. Fine-Grained Classi-\\nﬁcation of Offensive Language. In Proceedings of\\nGermEval 2018 (co-located with KONVENS), pages\\n38–44.\\nDietmar Schabus, Marcin Skowron, and Martin Trapp.\\n2017. One Million Posts: A Data Set of German On-\\nline Discussions. In Proceedings of the 40th Interna-\\ntional ACM SIGIR Conference on Research and De-\\nvelopment in Information Retrieval (SIGIR), pages\\n1241–1244, Tokyo, Japan.\\nMike Schuster and Kaisuke Nakajima. 2012. Japanese\\nand Korean voice search. In 2012 IEEE Interna-\\ntional Conference on Acoustics, Speech and Sig-\\nnal Processing (ICASSP), pages 5149–5152. ISSN:\\n2379-190X.\\nRico Sennrich, Barry Haddow, and Alexandra Birch.\\n2016. Neural Machine Translation of Rare Words\\nwith Subword Units. arXiv:1508.07909 [cs] .\\nArXiv: 1508.07909.\\nErik F. Tjong Kim Sang and Fien De Meulder.\\n2003. Introduction to the CoNLL-2003 Shared Task:\\nLanguage-Independent Named Entity Recognition.\\nIn Proceedings of the Seventh Conference on Nat-\\nural Language Learning at HLT-NAACL 2003 - Vol-\\nume 4, CONLL ’03, pages 142–147, USA. Associ-\\nation for Computational Linguistics. Event-place:\\nEdmonton, Canada.\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\\nKaiser, and Illia Polosukhin. 2017. Attention is All\\nyou Need. In I. Guyon, U. V . Luxburg, S. Bengio,\\nH. Wallach, R. Fergus, S. Vishwanathan, and R. Gar-\\nnett, editors, Advances in Neural Information Pro-\\ncessing Systems 30, pages 5998–6008. Curran Asso-\\nciates, Inc.\\nAntti Virtanen, Jenna Kanerva, Rami Ilo, Jouni Luoma,\\nJuhani Luotolahti, Tapio Salakoski, Filip Ginter, and\\nSampo Pyysalo. 2019. Multilingual is not enough:\\nBERT for Finnish. arXiv:1912.07076 [cs]. ArXiv:\\n1912.07076.\\nWietse de Vries, Andreas van Cranenburgh, Arianna\\nBisazza, Tommaso Caselli, Gertjan van Noord,\\nand Malvina Nissim. 2019. BERTje: A Dutch\\nBERT Model. arXiv:1912.09582 [cs]. ArXiv:\\n1912.09582.\\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\\nChaumond, Clement Delangue, Anthony Moi, Pier-\\nric Cistac, Tim Rault, R ´emi Louf, Morgan Funtow-\\nicz, and Jamie Brew. 2019. Huggingface’s trans-\\nformers: State-of-the-art natural language process-\\ning. CoRR, abs/1910.03771.\\nYang You, Jing Li, Sashank Reddi, Jonathan Hseu,\\nSanjiv Kumar, Srinadh Bhojanapalli, Xiaodan Song,\\nJames Demmel, Kurt Keutzer, and Cho-Jui Hsieh.\\n2020. Large Batch Optimization for Deep Learning:\\nTraining BERT in 76 minutes. arXiv:1904.00962\\n[cs, stat]. ArXiv: 1904.00962 version: 5.'), Document(metadata={'source': 'https://www.seatguru.com/airlines/Lufthansa/baggage.php', 'title': 'Lufthansa: Baggage Fees and Policy - SeatGuru', 'description': ' Before your next Lufthansa flight, be sure to visit our baggage guide to answer some of the most commonly asked questions.', 'language': 'No language found.'}, page_content='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\nLufthansa: Baggage Fees and Policy - SeatGuru\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSeat Maps\\nAirlines\\nCheap Flights\\n\\nComparison Charts\\n\\n\\n\\nShort-haul Economy Class\\nShort-haul First/Business Class\\nLong-haul Economy Class\\nPremium Economy Class\\nLong-haul Business Class\\nLong-haul First Class\\n\\n\\n\\nRental Cars\\nGuru Tips\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                    Love travel? Sign up for our free newsletter and get the latest news, insights, and money-saving tips.\\n\\t\\t    \\n\\n\\n\\t\\t\\t    By proceeding, you agree to our Privacy Policy and Terms of Use\\n\\n\\n\\n\\n\\n\\n\\n\\nSIGN UP\\n\\n\\nPlease enter a valid email address.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAirlines >Lufthansa >Baggage\\n\\n\\n\\n\\nLufthansa Baggage\\n \\n\\n\\n\\n\\n\\n\\n\\nOverview\\n\\nPlanes & Seat Maps \\nVIEW MORE PLANES  \\n\\nCheck-in\\nBaggage\\nInfants\\nMinors\\nPets\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCarry-On Allowance\\nFirst and Business class passengers are allowed two carry-ons. Economy class and Premium Economy class passengers are permitted one carry-on. Assistive devices and outer garments do not count as personal items. All carry-on luggage must fit in the overhead bin or under the seat in front of you and adhere to the following requirements:\\n\\nMaximum Dimensions of Carry-on Luggage: 55cm x 40cm x 23cm / 22in x 16in x 9in (length x width x height)\\nMaximum Weight of Carry-on Luggage: 8 Kg/18 lb\\n\\nFor more information, please consult Lufthansa\\'s Carry-on baggage information page.\\nREMINDER: Bulkhead seats do not have underseat storage during take-off and landing. Read Bulkheads Explained\\nChecked Baggage Allowance\\nIntercontinental flights\\nThe following basic regulations apply for the free baggage allowance on all intercontinental flights operated by Lufthansa:\\n\\nEconomy Class: 1 bag up to 23 kg (50 lb)\\nPremium Economy Class: 2 bags up to 23 kg (50 lb) each\\nBusiness Class: 2 bags up to 32 kg (70 lb) each\\nFirst Class: 3 bags up to 32 kg (70 lb) each\\n\\nFree baggage allowance for status customers on intercontinental flights\\nIf you are traveling on Lufthansa\\'s \"Economy Light\" product,\\xa0you are allowed one piece of carry-on baggage. This fare includes no free baggage allowance, however, a piece of checked baggage can be added to your booking for a fee.\\nFrequent Traveller\\n\\nEconomy Class: 2 bags up to 23 kg (50 lb)\\nPremium Economy Class: 2 bags up to 23 kg (50 lb) each\\nBusiness Class: 2 bags up to 32 kg (70 lb) each\\nFirst Class: 3 bags up to 32 kg (70 lb) each\\n\\nSenator / HON Circle Member / Star Alliance Gold member\\n\\nEconomy Class: 2 bags up to 23 kg (50 lb)\\nPremium Economy Class: 3 bags up to 23 kg (50 lb) each\\nBusiness Class: 3 bags up to 32 kg (70 lb) each\\nFirst Class: 4 bags up to 32 kg (70 lb) each\\nAdditional sports bag: + 1 golf bag\\n\\nEuropean flights\\nFor flights within Europe the free baggage rules are determined by the new Europe fare options:\\n\\nEconomy Light: 1 piece of carry-on baggage up to 8 kg only\\nEconomy Classic / Flex: 1 piece of carry-on baggage up to 8 kg; 1 bag up to 23 kg\\nBusiness Class: 2 pieces of carry-on baggage up to 8 kg; 2 bags up 32 kg\\n\\nFree baggage allowance for status customers on intercontinental flights\\nFrequent Traveller\\n\\nEconomy Class Light: Carry-on baggage only\\nEconomy Class Classic: 2 bags up to 23 kg (50 lb)\\nEconomy Class Flex: Flex: 2 bags up to 23 kg (50 lb)\\nBusiness Class: 2 bags up to 32 kg. (70 lb)\\n\\nSenator / HON Circle Member / Star Alliance Gold member\\n\\nEconomy Class Light: Carry-on baggage only\\nEconomy Class Classic: 2 bags up to 23 kg (50 lb)\\nEconomy Class Flex: Flex: 2 bags up to 23 kg (50 lb)\\nBusiness Class: 2 bags up to 32 kg. (70 lb)\\nAdditional sports bag: + 1 golf bag\\n\\nFree baggage dimensions\\nThe maximum size per piece of baggage, regardless of class, is 158 cm (width + height + depth). Items of baggage that are larger or heavier than the permitted dimensions and weight or that are additional to the free baggage allowance will be carried as excess baggage for a flat fee. Items that weigh more than 32 kg will be transported by air freight for a charge.\\nFor more information on baggage allowances for specific routes, visit  Lufthansa\\'s baggage site.\\nAdditionally, Lufthansa allows for the free transport of a pushchair or baby basket. Disabled passengers are allowed free transport of two wheelchairs or other mobility aids in addition to their free baggage allowance.\\nOverweight Baggage Fees\\nHeavier than free baggage allowance (24 – 32 kg): 50 € / USD 70 within Europe / between third countries and 100 € / USD 150 on intercontinental flights\\nExcess and Oversize Baggage Fees\\nEconomy and Premium Economy Class\\nLarger than free baggage allowance (from 159 cm): 100 € / USD 150 within Europe / between third countries and 200 € / USD 300 on intercontinental flights\\nLarger and heavier than free baggage allowance (24 – 32 kg and from 159 cm): 150 € / USD 220 within Europe / between third countries and 300 € / USD 450 on intercontinental flights.\\nBusiness and First Class\\nLarger than free baggage allowance (from 159 cm): 100 € / USD 150 within Europe / between third countries and 200 € / USD 300 on intercontinental flights.\\nSports Equipment\\nSports baggage is carried free of charge as part of your permitted free baggage allowance.\\nSports baggage must be registered up to 24 hours before departure via the reservation hotline.\\nIn addition to your free baggage allowance, you may take one piece of ski or snowboarding equipment free of charge on your Lufthansa flight. The maximum weight of an additional piece of ski or snowboarding equipment must be within the weight limits of the travel class you have booked. Please note that this rule only applies to flights operated by Lufthansa.\\nBecause of limited capacities and differing rules on partner airlines, Lufthansa cannot guarantee when you book that your ski baggage will be accepted in every case. Please register your ski bag up to 24 hours before departure via the reservation hotline, regardless of whether your ski bag is within your free baggage allowance or is being checked in as excess baggage.\\nCPAP Devices:\\nLufthansa will accept assistive devices with batteries as checked baggage as well as inside the cabin. Assistive devices with batteries include CPAP machines. Lufthansa will allow qualified individuals with a disability who are using FAA approved personal respirators/ventilators to bring their equipment, including non-spillable batteries, onboard the aircraft. You must have sufficient battery power for 150% of the maximum flight duration.\\n\\xa0\\n\\n\\n\\n \\n\\n\\n \\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSeatGuru was created to help travelers choose the best seats and in-flight amenities. \\n\\n\\n\\n\\nForum\\n\\nMobile\\n\\nFAQContact UsSite Map \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCopyright © TripAdvisor LLC, 2001 - 2025. All rights reserved.\\n\\nPrivacy and Cookie Statement\\nTerms and Conditions\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n')]\n"
     ]
    }
   ],
   "source": [
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'producer': 'pdfTeX-1.40.20', 'creator': 'TeX', 'creationdate': '2020-11-03T16:20:53+00:00', 'author': 'Branden Chan ; Stefan Schweter ; Timo Möller', 'moddate': '2020-11-03T16:20:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1', 'subject': 'COLING2020 2020', 'title': \"German's Next Language Model\", 'trapped': '/False', 'source': 'D:\\\\RAG_Travel_Assistance\\\\rag_travel_assistance\\\\docs\\\\GBERTPaper.pdf', 'total_pages': 9, 'page': 0, 'page_label': '1'}, page_content='Proceedings of the 28th International Conference on Computational Linguistics, pages 6788–6796\\nBarcelona, Spain (Online), December 8-13, 2020\\n6788\\nGerman’s Next Language Model\\nBranden Chan∗†, Stefan Schweter∗‡, Timo M¨oller†\\n†deepset\\n{branden.chan, timo.moeller}@deepset.ai\\n‡Bayerische Staatsbibliothek M¨unchen\\nDigital Library/Munich Digitization Center\\nstefan.schweter@bsb-muenchen.de\\nAbstract\\nIn this work we present the experiments which lead to the creation of our BERT and ELECTRA\\nbased German language models, GBERT and GELECTRA. By varying the input training data,\\nmodel size, and the presence of Whole Word Masking (WWM) we were able to attain SoTA\\nperformance across a set of document classiﬁcation and named entity recognition (NER) tasks\\nfor both models of base and large size. We adopt an evaluation driven approach in training these\\nmodels and our results indicate that both adding more data and utilizing WWM improve model\\nperformance. By benchmarking against existing German models, we show that these models\\nare the best German models to date. Our trained models will be made publicly available to the\\nresearch community.\\n1 Introduction\\nDeep transformer based language models have shown state-of-the-art results for various Natural Lan-\\nguage Processing tasks like text classiﬁcation, NER and question answering (Devlin et al., 2019). They\\nare pretrained, ﬁrst by feeding in large unlabeled text corpora before being ﬁne-tuned on the down-\\nstream task. In this work we present a set of German BERT and ELECTRA models, the best of which,\\nGELECTRALarge, signiﬁcantly improves upon state of the art performance on the GermEval18 hate\\nspeech detection task by about +4% / +2.5% for the coarse and ﬁne variants of the task respectively.\\nThis model also reaches SoTA on the GermEval14 NER task, outperforming the previous best by over\\n+4%. While performant, such models are prohibitively large for many and so we also present a new\\nGBERT model which matches deepset BERT, the previous best German BERT, in size but outperforms\\nit by +2.23% F1 averaged over three tasks.\\nIn the process of pretraining the language models, we also a) quantify the effect of increasing the\\ntraining data by an order of magnitude and b) verify that whole word masking has a positive effect on\\nBERT models.\\nBecause of the computational expense of training large language models from scratch, we adopt a\\ndownstream-oriented evaluation approach to ensure that we get the best performance from a limited\\nnumber of runs. This involves regularly checkpointing the model over the course of pretraining, evalu-\\nating these on a set of classiﬁcation and NER tasks and selecting as ﬁnal the checkpoint which shows\\nthe best performance. This stands in contrast to approaches where the ﬁnal model is simply saved after a\\nﬁxed number of steps. Our method is also an important tool in diagnosing pretraining and we hope that\\nit will be of use to other teams looking to train effective language models on a budget.\\n2 Related work\\nModern language model architectures are trained to build word representations that take into consid-\\neration the context around a given word. First versions such as ELMo (Peters et al., 2018), ULMFiT\\nThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details:\\nhttp://creativecommons.org/licenses/by/4.0/.\\n∗Equal contribution.'), Document(metadata={'producer': 'pdfTeX-1.40.20', 'creator': 'TeX', 'creationdate': '2020-11-03T16:20:53+00:00', 'author': 'Branden Chan ; Stefan Schweter ; Timo Möller', 'moddate': '2020-11-03T16:20:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1', 'subject': 'COLING2020 2020', 'title': \"German's Next Language Model\", 'trapped': '/False', 'source': 'D:\\\\RAG_Travel_Assistance\\\\rag_travel_assistance\\\\docs\\\\GBERTPaper.pdf', 'total_pages': 9, 'page': 1, 'page_label': '2'}, page_content='6789\\n(Howard and Ruder, 2018) and F LAIR (Akbik et al., 2018) are LSTM based and these were able to\\nset new performance benchmarks on downstream tasks like text classiﬁcation, PoS tagging and NER.\\nMore recent approaches use Transformer-based (Vaswani et al., 2017) architectures and examples in-\\nclude GPT-2 (Radford et al., 2019), BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), ALBERT\\n(Lan et al., 2020) and ELECTRA (Clark et al., 2020).\\nIn this work we focus on BERT and ELECTRA models. BERT uses a masked language modeling\\n(MLM) strategy to corrupt an input sentence by replacing some tokens with a [MASK] symbol. The\\nmodel is then trained to re-construct the original token. However, this method of training is somewhat\\nrestricted in that the model only learns from the masked out tokens which typically make up about 15%\\nof the input tokens.\\nELECTRA addresses this problem by introducing a new pretraining task called Replaced Token de-\\ntection. Instead of masking out tokens, a subset of the input tokens are substituted by a synthetically\\ngenerated token. The model is then trained to classify whether each input token is original or substituted,\\nthus allowing for gradient updates at every input position. Practically speaking, this is achieved by hav-\\ning a discriminator that performs the replaced token detection and a generator which provides plausible\\ntoken substitutes. These two components are trained jointly and are both Transformer based.\\nThe BERT model received an update when the original authors added Whole Word Masking1 whereby\\nmasking one subword token requires that all other tokens in the word are also masked out. The authors\\nreport that this method improves the training signal by removing the easiest cases and show that it im-\\nproves performance in their tasks.\\nThere is also a line of work that looks into bringing language modeling techniques that were ﬁrst\\ndeveloped on English to other languages. These include but are not limited to monolingual models\\nsuch as CamemBERT (Martin et al., 2020) and FlauBERT (Le et al., 2020) for French, Finnish BERT\\n(Virtanen et al., 2019) and German BERTs by DBMDZ 2 and deepset3. For a more comprehensive list,\\nsee (Nozza et al., 2020).\\nSome models are also capable of supporting multiple languages such as multilingual BERT\\n(mBERTBase) and XLM-RoBERTa (Conneau et al., 2019). Multilingual BERT is a multilingual model\\nfor 104 different languages 4 trained on Wikipedia dumps. The XLM-RoBERTa model is trained on\\n2.5TB of data from a cleaned Common Crawl corpus (Wenzek et al., 2020) for 100 different languages.\\nIt is worth emphasizing here that systems trained on naturally occurring data will learn pre-existing\\ncultural biases around gender (Bolukbasi et al., 2016), race and religion (Speer, 2017). Critical eval-\\nuation of machine learning methods is more important than ever as NLP is gaining broader adoption.\\nResearchers have been advocating for better documentation of decisions made during the construction of\\na dataset (Gebru et al., 2018), explicit statements of a dataset’s “ingredients” (Holland et al., 2018) and\\nrecognition of the dataset characteristics that may lead to exclusion, overgeneralisation and underexpo-\\nsure (Bender and Friedman, 2018). These topics will be addressed in Section 3.1.\\n3 Datasets\\n3.1 Pretraining Data\\nWe have available to us, a range of different German language corpora that we use in different combi-\\nnations for our model pretraining. OSCAR (Ortiz Su ´arez et al., 2019) is a set of monolingual corpora\\nextracted from Common Crawl. The Common Crawl texts are pre-processed (e.g. HTML entities are\\nremoved) and a language classiﬁcation model is used to sort texts by language. We use the unshufﬂed\\nversion of the German OSCAR corpus, resulting in 145GB of text. The Wikipedia dump for German is\\npreprocessed with the WikiExtractor5 script forming a corpus of size 6GB. The OPUS project 6 (Tiede-\\nmann, 2012) has collected texts from various domains such as movie subtitles, parliament speeches and\\n1https://github.com/google-research/bert/commit/0fce551\\n2https://github.com/dbmdz/berts\\n3https://deepset.ai/german-bert\\n4https://github.com/google-research/bert/blob/f39e88/multilingual.md\\n5https://github.com/attardi/wikiextractor\\n6http://opus.nlpl.eu'), Document(metadata={'producer': 'pdfTeX-1.40.20', 'creator': 'TeX', 'creationdate': '2020-11-03T16:20:53+00:00', 'author': 'Branden Chan ; Stefan Schweter ; Timo Möller', 'moddate': '2020-11-03T16:20:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1', 'subject': 'COLING2020 2020', 'title': \"German's Next Language Model\", 'trapped': '/False', 'source': 'D:\\\\RAG_Travel_Assistance\\\\rag_travel_assistance\\\\docs\\\\GBERTPaper.pdf', 'total_pages': 9, 'page': 2, 'page_label': '3'}, page_content='6790\\nDataset Size\\nOSCAR 145\\nOPUS 10\\nWikipedia 6\\nOpenLegalData 2.4\\nTable 1: The size of each dataset in gigabytes.\\nbooks and these comprise a collection of around 10GB. From Open Legal Data7 (Ostendorff et al., 2020)\\nthere is a dataset of about 2.4GB of German court decisions. Table 1 shows an overview over all datasets.\\nAs discussed in Section 2, our pretrained language models will learn pre-existing biases from the\\ntraining datasets. The main portion (89%) of our training data, namely the OSCAR dataset, uses texts\\nscraped from the internet, which is in some respects problematic. First off, this dataset contains a lot of\\nexplicit and indecent material. While we ﬁltered out many of these documents through keyword match-\\ning, we cannot guarantee that this method was successful in every case. Furthermore, many websites\\ncontain unveriﬁed information and any dataset containing this kind of text can lead to a skewed model\\nthat reﬂects commonly found lies and misconceptions. This includes gender, racial and religious biases\\nwhich are found in textual data of all registers and so we advise that anyone using our model to recognise\\nthat it will not always build true and accurate representation of real world concepts. We implore users\\nof the model to seriously consider these issues before deploying it in a production setting, especially\\nin situations where impartiality matter, such as journalism, and institutional decision making like job\\napplications or insurance assessments.\\n3.2 Downstream Data\\n3.2.1 GermEval18\\nFor text classiﬁcation we use GermEval18 (Coarse) and GermEval18 (Fine) which are both hate speech\\nclassiﬁcation tasks (Wiegand et al., 2018). GermEval18 (Coarse) requires a system to classify a tweet\\ninto one of two classes:OFFENSE if the tweet contains some form of offensive language, andOTHER if it\\ndoes not. GermEval18 (Fine) extends the coarse-grained task and contains four classes: OTHER for non-\\noffensive tweets as well as PROFANITY, INSULT and ABUSE which are all subclasses of OFFENSE\\nfrom the coarse variant of the task.\\n3.2.2 GermEval14\\nFor NER, we use the GermEval14 (Benikova et al., 2014) shared task. The data is sampled from German\\nWikipedia and News Corpora and contains over 31,000 sentences and 590,000 tokens. The dataset is\\none of the largest NER datasets for German and features an advanced annotation schema that allows for\\nnested annotations. The four main classes ( PERSON, ORGANISATION, LOCATION and OTHER) each\\nhave part and derivative variants (e.g. LOCpart or PERderiv) resulting in 12 classes in total.\\n4 Training\\n4.1 Method\\nTo train our German BERT and ELECTRA we use the Tensorﬂow training scripts from the ofﬁcial\\nrepositories8. We train models that match the size of the original BERT Base, BERTLarge, ELECTRABase\\nand ELECTRALarge. The hyperparameters used for training can be found in Table 2. The base models\\nwere trained on single Google Cloud TPUs v3 (8 cores) while large models were trained on pods of 16\\nTPUs v3 (128 cores).\\n7http://openlegaldata.io/research/2019/02/19/court-decision-dataset.html\\n8https://github.com/google-research/bert and https://github.com/google-research/\\nelectra'), Document(metadata={'producer': 'pdfTeX-1.40.20', 'creator': 'TeX', 'creationdate': '2020-11-03T16:20:53+00:00', 'author': 'Branden Chan ; Stefan Schweter ; Timo Möller', 'moddate': '2020-11-03T16:20:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1', 'subject': 'COLING2020 2020', 'title': \"German's Next Language Model\", 'trapped': '/False', 'source': 'D:\\\\RAG_Travel_Assistance\\\\rag_travel_assistance\\\\docs\\\\GBERTPaper.pdf', 'total_pages': 9, 'page': 3, 'page_label': '4'}, page_content='6791\\nGBERTBase GBERTLarge GELECTRABase GELECTRALarge\\nmax sequence length 512 512 512 512\\nbatch size 128 2048 256 1024\\nwarmup steps (k) 10 10 10 30\\nlearning rate 1e-04 1e-04 2e-04 2e-4\\ncheckpoint every (k) 100 100 76.6 100\\nmax train steps (k) 4000 1000 766 1000\\nlayers 12 24 12 24\\nhidden states 768 1024 768 1024\\nattention heads 12 16 12 16\\nvocab size (k) 31 31 31 31\\ntrain time (days) 7 11 8 7\\nTable 2: Hyperparameters for language model pretraining.\\n4.2 Models\\nIn total, we trained 7 separate models with different combinations of data and model size as well as\\nWhole Word Masking (WWM) for BERT models. The German DBMDZ BERT Base, is the same size\\nas BERTBase and was trained using the OPUS and Wikipedia corpora. It serves as our baseline model.\\nWe train four BERT variants of it, each referred to as GBERT, each using the same cased vocabulary as\\nDBMDZ BERTBase. These match BERT Base in size unless they have the ”Large” sufﬁx, in which case\\nthey match BERTLarge:\\n•GBERTData - trained on all available data without Whole Word Masking\\n•GBERTWWM - trained on the same data as DBMDZ BERTBase but uses Whole Word Masking\\n•GBERTData + WWM - trained on all available data and uses Whole Word Masking\\n•GBERTLarge - trained on all available data and uses Whole Word Masking\\nWe also trained three ELECTRA variants of DBMDZ BERT Base, each referred to as GELECTRA\\nmodels, which also match the size of the original ELECTRA Base unless they have the ”Large” sufﬁx in\\nwhich case they match ELECTRALarge:\\n•GELECTRA - trained on same data as DBMDZBase BERT\\n•GELECTRAData - trained on all available data\\n•GELECTRALarge - trained on all available data\\nThe best models of each architecture and size are uploaded to the Hugging Face model hub 9 as\\ndeepset/gbert-base, deepset/gbert-large, deepset/gelectra-base and deepset/gelectra-large.\\n5 Evaluation\\nIn our approach, models are evaluated continuously during pretraining. Model checkpoints are saved at\\nregular intervals and converted into PyTorch models using Hugging Face’s Transformers library (Wolf\\net al., 2019). Using the FARM framework 10, we evaluate the performance of each checkpoint on Ger-\\nmEval18 (Coarse) and GermEval18 (Fine) which are both hate speech classiﬁcation tasks (Wiegand et\\nal., 2018). Using Hugging Face’s Transformers we also evaluate on GermEval14 (Benikova et al., 2014)\\nwhich is a NER task.\\n9https://huggingface.co/models\\n10https://github.com/deepset-ai/FARM'), Document(metadata={'producer': 'pdfTeX-1.40.20', 'creator': 'TeX', 'creationdate': '2020-11-03T16:20:53+00:00', 'author': 'Branden Chan ; Stefan Schweter ; Timo Möller', 'moddate': '2020-11-03T16:20:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1', 'subject': 'COLING2020 2020', 'title': \"German's Next Language Model\", 'trapped': '/False', 'source': 'D:\\\\RAG_Travel_Assistance\\\\rag_travel_assistance\\\\docs\\\\GBERTPaper.pdf', 'total_pages': 9, 'page': 4, 'page_label': '5'}, page_content='6792\\nGermEval18 (Coarse) GermEval18 (Fine) GermEval14\\nType Classiﬁcation Classiﬁcation NER\\nTrain Samples 4509 4509 24002\\nDev Samples 501 501 2200\\nTest Samples 3533 3533 5100\\nClasses 2 4 12\\nMax Epochs 5 5 3\\nMax Train Steps 705 705 4500\\nEvaluation Every 50 steps 50 steps 1500 steps\\nLearning Rate 5e-06 5e-06 5e-05\\nBatch Size 32 32 16\\nMax Seq Len 150 150 128\\nMetric F1 (macro) F1 (macro) F1 (micro)\\nTable 3: Details of the downstream tasks and hyperparameters for model ﬁnetuning for all three tasks.\\nIn BERT, the vector corresponding to the [CLS] token serves as a representation of the whole input\\nsequence, while in ELECTRA, all word vectors are combined through a feed forward layer. In both\\ncases, this input sequence representation is passed through a single layer Neural Network in order to\\nperform prediction. In the NER task, each vector corresponding to the ﬁrst token in a word is passed\\nthrough a single layer Neural Network and the resulting prediction is applied to the whole word.\\nEach checkpoint is evaluated 3 times on each document classiﬁcation task since we observed signif-\\nicant variance across different runs. Each of these runs is performed with early stopping and a differ-\\nent seed each time. For NER, the model is evaluated just once without early stopping. The reported\\nperformance is the average of the single best run for GermEval18 (Coarse), GermEval18 (Fine) and\\nGermEval14. Table 3 summarizes the most important details and parameters of each task. For all exper-\\niments, we use an Nvidia V100 GPU to accelerate training. For each model, we choose the checkpoint\\nthat shows the best performance.\\nFor comparison, we also run this evaluation pipeline on the two publicly available German BERT\\nmodels (deepset German BERT Base and DBMDZ German BERT Base) as well as multilingual models\\nsuch as mBERTBase and XLM-RoBERTaLarge.\\n6 Results\\nThe downstream performance graphs in Figure 1 show that the models are capable of learning with most\\nof the gains being made in the ﬁrst phase of training and more incremental gains coming later. The best\\ncheckpoints come at different points for different models as can be seen in Table 4.\\nIn Table 5 are the evaluation results for each model’s best checkpoint for each of the three downstream\\ntasks with comparison to benchmark models and previous SoTA results. For GermEval18, results from\\nthe best-performing systems are reported (Wiegand et al., 2018). For GermEval14 we report the result\\nthat can be achieved using the FLAIR framework (Akbik et al., 2019).\\nSteps (k)\\nGBERTData 3900\\nGBERTWWM 1500\\nGBERTData + WWM 2000\\nGBERTLarge 900\\nGELECTRA 766\\nGELECTRAData 766\\nGELECTRALarge 1000\\nTable 4: Best checkpoint of each trained model.'), Document(metadata={'producer': 'pdfTeX-1.40.20', 'creator': 'TeX', 'creationdate': '2020-11-03T16:20:53+00:00', 'author': 'Branden Chan ; Stefan Schweter ; Timo Möller', 'moddate': '2020-11-03T16:20:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1', 'subject': 'COLING2020 2020', 'title': \"German's Next Language Model\", 'trapped': '/False', 'source': 'D:\\\\RAG_Travel_Assistance\\\\rag_travel_assistance\\\\docs\\\\GBERTPaper.pdf', 'total_pages': 9, 'page': 5, 'page_label': '6'}, page_content='6793\\nFigure 1: The F1 performance of each model averaged over the three downstream tasks over the course\\nof language model pretraining.\\nIn GermEval18 (Coarse), GBERTData + WWM, XLM-RobertaLarge, GBERTLarge and GELECTRALarge\\nall improve upon the previous SoTA. GELECTRALarge does so with the largest margin reaching a score\\nthat is +3.93% better. In GermEval18 (Fine), XLM-Roberta Large beats the previous best by +1.39% and\\nGELECTRALarge sets a new SoTA that is better than the previous by +2.45%. In GermEval14, all 7\\ntrained models exceed the previous SoTA, with GELECTRA Large showing a +4.3% improvement over\\nthe previous best.\\nThese results indicate that adding extra data gives a consistent but modest performance boost to our\\nlanguage models. GBERT Data outperforms DBMDZ BERTBase by +0.25%, GBERTData + WWM outper-\\nforms GBERTWWM by +0.93% and GELECTRA Data outperforms GELECTRA by +1.59%. For the\\nBERT models, Whole Word Masking also shows a consistent positive impact with GBERTWWM outper-\\nforming DBMDZ BERTBase by +1.70% and GBERTData + WWM outperforming GBERTData by +2.38%.\\n7 Discussion\\n7.1 Model Size\\nThe large models that we train show much stronger performance than the base models. GBERTLarge out-\\nperforms GBERTData + WWM by +2.33% averaged F1 and GELECTRALarge outperforms GELECTRAData'), Document(metadata={'producer': 'pdfTeX-1.40.20', 'creator': 'TeX', 'creationdate': '2020-11-03T16:20:53+00:00', 'author': 'Branden Chan ; Stefan Schweter ; Timo Möller', 'moddate': '2020-11-03T16:20:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1', 'subject': 'COLING2020 2020', 'title': \"German's Next Language Model\", 'trapped': '/False', 'source': 'D:\\\\RAG_Travel_Assistance\\\\rag_travel_assistance\\\\docs\\\\GBERTPaper.pdf', 'total_pages': 9, 'page': 6, 'page_label': '7'}, page_content='6794\\nParams GermEval18 (Coarse)GermEval18 (Fine) GermEval14 Averaged F1\\nDBMDZ BERTBase 110m 75.23 47.39 87.90 69.72\\ndeepset BERTBase 110m 74.7 48.8 86.87 70.12\\nmBERTBase 172m 70.00 45.20 87.44 67.55\\nXLM-RobertaLarge 550m 78.38 54.1 87.07 73.18\\nGBERTData 110m 74.51 48.01 87.41 69.97\\nGBERTWWM 110m 76.48 49.99 87.80 71.42\\nGBERTData + WWM 110m 78.17 50.90 87.98 72.35\\nGBERTLarge 335m 80.08 52.48 88.16 73.57\\nGELECTRA 110m 76.02 42.22 86.02 68.09\\nGELECTRAData 110m 76.59 46.28 86.02 69.63\\nGELECTRALarge 335m 80.70 55.16 88.95 74.94\\nPrevious SoTA 76.77 (TU Wien) 52.71 (uhhLT) 84.65 (FLAIR)\\nTable 5: Downstream evaluation results for the best checkpoints of each GBERT and GELECTRA model\\ncompared to a set of benchmark models. For GermEval18 we report scores for the best-performing\\nsystems (Wiegand et al., 2018), and the result reported by F LAIR framework (Akbik et al., 2019) for\\nGermEval14.\\nby +5.31%. It must be noted however, that their differing training regimes mean that the large models\\nare trained on many more tokens than their base counterparts. In future, we would also be interested in\\ntraining larger models with less data in order to better quantify the gains that come from model size and\\nthe gains that come from the extra data.\\n7.2 Training Length\\nFrom the downstream evaluation graphs in Figure 1, it is clear that the models gain most of their perfor-\\nmance after a relatively short amount of training steps. GBERT WWM and GBERTData + WWM both show\\nan upward trend in the second half of model training suggesting they could still beneﬁt from continuing\\ntraining. There is also a clear upward trend over the course of GELECTRA and GELECTRAData’s train-\\ning suggesting these models are undertrained. It should also be noted that none of the models exhibit any\\nclear signs of overﬁtting or performance degradation and may improve with further training.\\n7.3 ELECTRA Efﬁciency\\nOne of the central claims of the ELECTRA paper is that it is capable of learning more efﬁciently\\nthan MLM based Language Models. This is exempliﬁed by the comparison of GBERT Large and\\nGELECTRALarge. By the end of their 1 million steps of training, GELECTRA Large has only seen half\\nthe number of tokens that GBERT Large due to its smaller batch size and yet outperforms it by +1.47%\\naveraged F1.\\n7.4 Instabilities\\nThe dip in performance around 2 million steps for the base sized GBERT models (See Figure 1) happens\\nto coincide with our training regime whereby the model training is stopped, saved and then reloaded at 2\\nmillion steps. While we suspect that these two events are related, it was beyond the scope of this project\\nto investigate the exact reasons.\\n8 Conclusion\\nThe set of German models which we trained vary in terms of training regime and model architecture. We\\nhope that the results that we present here will serve as important data points to other NLP practitioners\\nwho are looking to train language models from scratch but are limited by compute. Our experiments\\nshould give other teams a sense of the batch sizes and training lengths that make for efﬁcient model\\ntraining. On top of this, we also present a set of GELECTRA and GBERT models which, according to\\nour evaluations, set new SoTA performance for both large and base sized models on GermEval18 and\\nGermEval14.'), Document(metadata={'producer': 'pdfTeX-1.40.20', 'creator': 'TeX', 'creationdate': '2020-11-03T16:20:53+00:00', 'author': 'Branden Chan ; Stefan Schweter ; Timo Möller', 'moddate': '2020-11-03T16:20:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1', 'subject': 'COLING2020 2020', 'title': \"German's Next Language Model\", 'trapped': '/False', 'source': 'D:\\\\RAG_Travel_Assistance\\\\rag_travel_assistance\\\\docs\\\\GBERTPaper.pdf', 'total_pages': 9, 'page': 7, 'page_label': '8'}, page_content='6795\\nAcknowledgements\\nWe would like to thank the deepset team, especially Malte Pietsch and Tanay Soni for their regular\\nsparring and their effort maintaining FARM. Thanks to Zak Stone, Jonathan Caton and everyone at the\\nGoogle TensorFlow Research Cloud team for their advice and for providing us with the access to and\\ncredits for the TPU pods that we used for pretraining. We would also like to thank Nikhil Dinesh from the\\nAWS Activate program as well as Nvidia’s Inception program for providing us with the EC2 instances\\nand credits that allowed us to do large scale evaluation of our models. Thanks also to Pedro Javier Ortiz\\nSu´arez and the OSCAR corpus team for giving us access to their dataset. And thanks to Malte Ostendorff,\\nco-founder of Open Justice e.V ., whose team created Open Legal Data.\\nReferences\\nAlan Akbik, Duncan Blythe, and Roland V ollgraf. 2018. Contextual string embeddings for sequence labeling. In\\nCOLING 2018, 27th International Conference on Computational Linguistics, pages 1638–1649.\\nAlan Akbik, Tanja Bergmann, Duncan Blythe, Kashif Rasul, Stefan Schweter, and Roland V ollgraf. 2019. FLAIR:\\nAn easy-to-use framework for state-of-the-art NLP. In Proceedings of the 2019 Conference of the North Amer-\\nican Chapter of the Association for Computational Linguistics (Demonstrations), pages 54–59, Minneapolis,\\nMinnesota, June. Association for Computational Linguistics.\\nEmily M. Bender and Batya Friedman. 2018. Data statements for natural language processing: Toward mitigating\\nsystem bias and enabling better science. Transactions of the Association for Computational Linguistics, 6:587–\\n604.\\nDarina Benikova, Chris Biemann, Max Kisselew, and Sebastian Pad´o. 2014. Germeval 2014 named entity recog-\\nnition: Companion paper. Proceedings of the KONVENS GermEval Shared Task on Named Entity Recognition,\\nHildesheim, Germany, pages 104–112.\\nTolga Bolukbasi, Kai-Wei Chang, James Y Zou, Venkatesh Saligrama, and Adam T Kalai. 2016. Man is to\\ncomputer programmer as woman is to homemaker? debiasing word embeddings. In D. D. Lee, M. Sugiyama,\\nU. V . Luxburg, I. Guyon, and R. Garnett, editors,Advances in Neural Information Processing Systems 29, pages\\n4349–4357. Curran Associates, Inc.\\nKevin Clark, Minh-Thang Luong, Quoc V . Le, and Christopher D. Manning. 2020. Electra: Pre-training text\\nencoders as discriminators rather than generators. In International Conference on Learning Representations.\\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco\\nGuzm´an, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Unsupervised cross-\\nlingual representation learning at scale.\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirec-\\ntional transformers for language understanding. In Proceedings of the 2019 Conference of the North American\\nChapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and\\nShort Papers), pages 4171–4186, Minneapolis, Minnesota, June. Association for Computational Linguistics.\\nTimnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, III Daum ´e,\\nHal, and Kate Crawford. 2018. Datasheets for Datasets. arXiv e-prints, page arXiv:1803.09010, March.\\nSarah Holland, Ahmed Hosny, Sarah Newman, Joshua Joseph, and Kasia Chmielinski. 2018. The Dataset Nu-\\ntrition Label: A Framework To Drive Higher Data Quality Standards. arXiv e-prints, page arXiv:1805.03677,\\nMay.\\nJeremy Howard and Sebastian Ruder. 2018. Universal language model ﬁne-tuning for text classiﬁcation. In\\nProceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long\\nPapers), pages 328–339, Melbourne, Australia, July. Association for Computational Linguistics.\\nZhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. 2020.\\nAlbert: A lite bert for self-supervised learning of language representations. In International Conference on\\nLearning Representations.\\nHang Le, Lo¨ıc Vial, Jibril Frej, Vincent Segonne, Maximin Coavoux, Benjamin Lecouteux, Alexandre Allauzen,\\nBenoit Crabb ´e, Laurent Besacier, and Didier Schwab. 2020. FlauBERT: Unsupervised language model pre-\\ntraining for French. In Proceedings of The 12th Language Resources and Evaluation Conference, pages 2479–\\n2490, Marseille, France, May. European Language Resources Association.'), Document(metadata={'producer': 'pdfTeX-1.40.20', 'creator': 'TeX', 'creationdate': '2020-11-03T16:20:53+00:00', 'author': 'Branden Chan ; Stefan Schweter ; Timo Möller', 'moddate': '2020-11-03T16:20:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) kpathsea version 6.3.1', 'subject': 'COLING2020 2020', 'title': \"German's Next Language Model\", 'trapped': '/False', 'source': 'D:\\\\RAG_Travel_Assistance\\\\rag_travel_assistance\\\\docs\\\\GBERTPaper.pdf', 'total_pages': 9, 'page': 8, 'page_label': '9'}, page_content='6796\\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke\\nZettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv\\npreprint arXiv:1907.11692.\\nLouis Martin, Benjamin Muller, Pedro Javier Ortiz Su´arez, Yoann Dupont, Laurent Romary, ´Eric Villemonte de la\\nClergerie, Djam´e Seddah, and Benoˆıt Sagot. 2020. Camembert: a tasty french language model. In Proceedings\\nof the 58th Annual Meeting of the Association for Computational Linguistics.\\nDebora Nozza, Federico Bianchi, and Dirk Hovy. 2020. What the [MASK]? Making Sense of Language-Speciﬁc\\nBERT Models. arXiv e-prints, page arXiv:2003.02912, March.\\nPedro Javier Ortiz Su´arez, Benoˆıt Sagot, and Laurent Romary. 2019. Asynchronous Pipeline for Processing Huge\\nCorpora on Medium to Low Resource Infrastructures. In Piotr Ba ´nski, Adrien Barbaresi, Hanno Biber, Evelyn\\nBreiteneder, Simon Clematide, Marc Kupietz, Harald L¨ungen, and Caroline Iliadi, editors, 7th Workshop on the\\nChallenges in the Management of Large Corpora (CMLC-7), Cardiff, United Kingdom, July. Leibniz-Institut\\nf¨ur Deutsche Sprache.\\nMalte Ostendorff, Till Blume, and Saskia Ostendorff. 2020. Towards an open platform for legal information. In\\nProceedings of the ACM/IEEE Joint Conference on Digital Libraries in 2020, JCDL ’20, page 385–388, New\\nYork, NY , USA. Association for Computing Machinery.\\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettle-\\nmoyer. 2018. Deep contextualized word representations. In Proceedings of the 2018 Conference of the North\\nAmerican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1\\n(Long Papers), pages 2227–2237, New Orleans, Louisiana, June. Association for Computational Linguistics.\\nAlec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models\\nare unsupervised multitask learners.\\nRobyn Speer. 2017. Conceptnet numberbatch 17.04: better, less-stereotyped word vectors.\\nJ¨org Tiedemann. 2012. Parallel data, tools and interfaces in opus. In Nicoletta Calzolari (Conference Chair),\\nKhalid Choukri, Thierry Declerck, Mehmet Ugur Dogan, Bente Maegaard, Joseph Mariani, Jan Odijk, and\\nStelios Piperidis, editors, Proceedings of the Eight International Conference on Language Resources and Eval-\\nuation (LREC’12), Istanbul, Turkey, may. European Language Resources Association (ELRA).\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser,\\nand Illia Polosukhin. 2017. Attention is all you need. In I. Guyon, U. V . Luxburg, S. Bengio, H. Wallach,\\nR. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems 30,\\npages 5998–6008. Curran Associates, Inc.\\nAntti Virtanen, Jenna Kanerva, Rami Ilo, Jouni Luoma, Juhani Luotolahti, Tapio Salakoski, Filip Ginter, and\\nSampo Pyysalo. 2019. Multilingual is not enough: BERT for Finnish. arXiv e-prints, page arXiv:1912.07076,\\nDecember.\\nGuillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzm ´an, Armand\\nJoulin, and Edouard Grave. 2020. CCNet: Extracting high quality monolingual datasets from web crawl\\ndata. In Proceedings of The 12th Language Resources and Evaluation Conference, pages 4003–4012, Mar-\\nseille, France, May. European Language Resources Association.\\nMichael Wiegand, Melanie Siegel, and Josef Ruppenhofer. 2018. Overview of the germeval 2018 shared task on\\nthe identiﬁcation of offensive language. In Proceedings of GermEval 2018, 14th Conference on Natural Lan-\\nguage Processing (KONVENS 2018), Vienna, Austria – September 21, 2018, pages 1 – 10. Austrian Academy\\nof Sciences, Vienna, Austria.\\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac,\\nTim Rault, R’emi Louf, Morgan Funtowicz, and Jamie Brew. 2019. Huggingface’s transformers: State-of-the-\\nart natural language processing. ArXiv, abs/1910.03771.'), Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2020-12-04T02:10:31+00:00', 'author': '', 'keywords': '', 'moddate': '2020-12-04T02:10:31+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'D:\\\\RAG_Travel_Assistance\\\\rag_travel_assistance\\\\docs\\\\GottbertPaper.pdf', 'total_pages': 6, 'page': 0, 'page_label': '1'}, page_content='GottBERT: a pure German Language Model\\nRaphael Scheible1, Fabian Thomczyk1, Patric Tippmann1, Victor Jaravine1 and Martin Boeker1\\n{raphael.scheible,thomczyk,tippmann,victor.zharavin,martin.boeker}\\n@imbi.uni-freiburg.de\\n1 Institute of Medical Biometry and Statistics, Medical Center, Faculty of Medicine, University of Freiburg\\nAbstract\\nLately, pre-trained language models advanced\\nthe ﬁeld of natural language processing (NLP).\\nThe introduction of Bidirectional Encoders for\\nTransformers (BERT) and its optimized ver-\\nsion RoBERTa have had signiﬁcant impact and\\nincreased the relevance of pre-trained models.\\nFirst, research in this ﬁeld mainly started on\\nEnglish data followed by models trained with\\nmultilingual text corpora. However, current re-\\nsearch shows that multilingual models are in-\\nferior to monolingual models. Currently, no\\nGerman single language RoBERTa model is\\nyet published, which we introduce in this work\\n(GottBERT). The German portion of the OS-\\nCAR data set was used as text corpus. In\\nan evaluation we compare its performance\\non the two Named Entity Recognition (NER)\\ntasks Conll 2003 and GermEval 2014 as well\\nas on the text classiﬁcation tasks GermEval\\n2018 (ﬁne and coarse) and GNAD with ex-\\nisting German single language BERT models\\nand two multilingual ones. GottBERT was\\npre-trained related to the original RoBERTa\\nmodel using fairseq. All downstream tasks\\nwere trained using hyperparameter presets\\ntaken from the benchmark of German BERT.\\nThe experiments were setup utilizing FARM.\\nPerformance was measured by the F1 score.\\nGottBERT was successfully pre-trained on a\\n256 core TPU pod using the RoBERTa BASE\\narchitecture. Even without extensive hyper-\\nparameter optimization, in all NER and one\\ntext classiﬁcation task, GottBERT already out-\\nperformed all other tested German and multi-\\nlingual models. In order to support the Ger-\\nman NLP ﬁeld, we publish GottBERT under\\nthe AGPLv3 license.\\n1 Introduction\\nThe computation of contextual pre-trained word\\nrepresentation is the current trend of neural net-\\nworks in natural language processing (NLP).\\nThis trend has a long history, starting with non-\\ncontextualized word representations of which the\\nmost prominent are word2vec (Goldberg and Levy,\\n2014) , GloVe (Pennington et al., 2014) and fast-\\nText (Joulin et al., 2016, 2017; Bojanowski et al.,\\n2017; Mikolov et al., 2018), evolving to deep con-\\ntextualized models such as ELmO (Alsentzer et al.,\\n2019) and ﬂair (Akbik et al., 2019). Most recently,\\nthe ﬁeld of NLP experienced remarkable progress,\\nby the use of Transformer (Vaswani et al., 2017)\\nbased approaches. Especially Bidirectional En-\\ncoder Representations from Transformers (BERT)\\n(Devlin et al., 2019) impacted the ﬁeld which sub-\\nsequently was robustly optimized to RoBERTa (Liu\\net al., 2019). These Transformer based approaches\\nuse large scale pre-trained language models. For\\napplication, such models are ﬁne tuned by a super-\\nvised training on the speciﬁc downstream task lead-\\ning to performance improvements in many tasks.\\nOn the other hand, the computation of the language\\nmodel is performed unsupervised. Large text blobs\\nare required for training and strong hardware such\\nas hundreds of GPUs (Martin et al., 2020) or TPUs\\n(You et al., 2020). Initially, most of the research\\ntook place in English followed by multilingual ap-\\nproaches (Conneau et al., 2019). Although, mul-\\ntilingual approaches were trained on large texts\\nof many languages, they were outperformed by\\nsingle language models (de Vries et al., 2019; Mar-\\ntin et al., 2020; Le et al., 2020; Delobelle et al.,\\n2020). Single language models trained with the\\nOpen Super-large Crawled ALMAnaCH coRpus\\n(OSCAR) (Ortiz Su´arez et al., 2020) showed good\\nperformance due to the size and variance of the OS-\\nCAR (Martin et al., 2020; Delobelle et al., 2020).\\nFollowing this ongoing trend, we pre-train the ﬁrst\\nGerman RoBERTa single language model with the\\nGerman portion of the OSCAR - the German OS-\\nCAR text trained BERT (GottBERT). In an evalua-\\ntion we compare its performance on the two named\\narXiv:2012.02110v1  [cs.CL]  3 Dec 2020'), Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2020-12-04T02:10:31+00:00', 'author': '', 'keywords': '', 'moddate': '2020-12-04T02:10:31+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'D:\\\\RAG_Travel_Assistance\\\\rag_travel_assistance\\\\docs\\\\GottbertPaper.pdf', 'total_pages': 6, 'page': 1, 'page_label': '2'}, page_content='entity recognition tasks Conll 2003 and GermEval\\n2014, as well as on the two text classiﬁcation tasks\\nGermEval 2018 and GNAD with existing German\\nsingle language BERT models and two multilingual\\nones.\\n2 Related Work\\nMost recently Transformer based models widely\\nimpacted the ﬁeld of NLP. From neural translation\\n(Ott et al., 2018; Ng et al., 2019) to generative\\nlanguage models as GPT2 (Radford et al., 2019),\\nremarkable performance gains were achieved. With\\nBERT, an approach facilitating pre-trained trans-\\nformer based models was introduced. Fine-tuned\\non downstream tasks, BERT based approaches\\nimproved the performance of several NLP tasks\\n(Devlin et al., 2019; Liu et al., 2019). BERT\\nmodels though, were ﬁrst released as single lan-\\nguage models in English based on 16GB of raw\\ntext and as the multilingual model mBERT based\\non Wikipedia dumps in about 100 languages (De-\\nvlin, 2018). These models were followed by sin-\\ngle language models for several languages: Bertje\\n(de Vries et al., 2019) for Dutch, FinBERT (Virta-\\nnen et al., 2019) for Finish, German BERT 1 and a\\nGerman BERT from the MDZ Digital Library team\\nat the Bavarian State Library to which we refer as\\ndbmz BERT in this paper 2. German BERT was\\ntrained using 12GB of raw text data basing on Ger-\\nman Wikipedia (6GB), the OpenLegalData dump\\n(2.4GB) and news articles (3.6GB). dbmz BERT\\nused as source data a German Wikipedia dump, EU\\nBookshop corpus, Open Subtitles, CommonCrawl,\\nParaCrawl and News Crawl which sums up to a\\ndataset of 16GB. With the release of RoBERTa a\\nnew standard for raw text size was set as it was\\ntrained on 160GB of raw English text. Further,\\nRoBERTa enhances the original BERT approach\\nby removing segment embeddings, next sentence\\nprediction and improved hyperparameters. Ad-\\nditionally, instead of using wordpiece (Schuster\\nand Nakajima, 2012) tokenization, RoBERTa uti-\\nlizes GPT2’s byte pair encoding (BPE) (Radford\\net al., 2019) with the beneﬁt that language-speciﬁc\\ntokenizers are not required. Other than mBERT,\\nthe multilingual XLM-RoBERTa (Conneau et al.,\\n2019) was trained on 2.5TB of ﬁltered Common-\\nCrawl data. CamemBERT is a French RoBERTa\\n1https://deepset.ai/german-bert\\n2https://github.com/dbmdz/berts#\\ngerman-bert\\nmodel that was trained on the OSCAR and uses\\nsentencepiece (Kudo and Richardson, 2018) BPE.\\nFurther, they pre-trained a model with 4GB of the\\nFrench OSCAR portion and another model with\\n4GB of the French Wikipedia. The comparison\\nof these models using downstream tasks shows\\nthat high text variance leads to better results. Um-\\nBERTo3 is an Italian RoBERTa model, similarly\\ndesigned as CamemBERT. RobBERT, the Dutch\\nsingle language RoBERTa, was trained on 39GB\\nof the Dutch portion of the OSCAR and outper-\\nformed Bertje. A more recent version of RobBert\\nshowed the performance gains of language speciﬁc\\nBPE compared to the English based GPT2 BPE\\nin downstream tasks. Most recently FlauBERT\\n(Le et al., 2020) for French was released trained\\non 71GB data. They cleaned a 270GB corpus of\\nmixed sources by ﬁltering out meaningless con-\\ntent and Unicode-normalization. Data was pre-\\ntokenized by moses (Koehn et al., 2007) and en-\\ncoded by fastBPE4 which is an implementation of\\nSennrich et al. (2016). Following the approach of\\nutilizing the OSCAR, we computed the German\\nOSCAR text trained BERT (GottBERT). However,\\nthe drawback of BERT approaches is the computa-\\ntional power requirement. Multiple GPUs or TPUs\\nwere used for pre-training. All listed RoBERTa\\nbased models were computed on GPUs whereas\\nGottBERT is the ﬁrst published RoBERTa model\\npre-trained on TPUs.\\n3 GottBERT\\nGottBERT bases on the robustly optimized BERT\\narchitecture RoBERTa, which was pre-trained with\\nthe German portion of the OSCAR using the fairseq\\nframework (Ott et al., 2019).\\nTraining Data\\nThe GottBERT model is trained on the German\\nportion of the OSCAR, a recently published large\\nmultilingual text corpus extracted from Common\\nCrawl. The German data portion of the OSCAR\\nmeasures 145GB of text containing approximately\\n21.5 billion words in approximately 459 million\\ndocuments (one document per line).\\nPre-processing\\nOriginally, RoBERTa uses GPT2 (Radford et al.,\\n2019) byte pair encoding to segment the input into\\n3https://github.com/\\nmusixmatchresearch/umberto\\n4https://github.com/glample/fastBPE'), Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2020-12-04T02:10:31+00:00', 'author': '', 'keywords': '', 'moddate': '2020-12-04T02:10:31+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'D:\\\\RAG_Travel_Assistance\\\\rag_travel_assistance\\\\docs\\\\GottbertPaper.pdf', 'total_pages': 6, 'page': 2, 'page_label': '3'}, page_content='subword units. Therefore, no pre-tokenization is\\nrequired and thus no language-speciﬁc tokenizer\\nas e.g. moses (Koehn et al., 2007) must be used.\\nIts original vocabulary was computed on English\\ndata. For GottBERT we computed a vocabulary\\nof 52k subword tokens based on 40 GB randomly\\nsampled documents of the German OSCAR portion.\\nCompared to the original GPT2 tokenizer, which\\nwas trained on English data, this leads to a 40%\\nsmaller size of the binary data which are fed into\\nfairseq (Ott et al., 2019). Furthermore, according\\nto Delobelle et al. (2020), it leads to a performance\\nincrease.\\nPre-training\\nUsing fairseq, we pre-trained GottBERT on a 256\\ncore TPU pod using the RoBERTa BASE architec-\\nture. We trained the model in 100k update steps\\nusing a batch size of 8k. A 10k iteration warmup\\nof the learning rate to a peak of 0.0004 was applied,\\nfrom which the learning rate polynomially decayed\\nto zero.\\nDownstream Tasks\\nBased on the pre-trained BERT models, several\\ndownstream tasks were trained. The Framework for\\nAdapting Representation Models (FARM)5 already\\ncomes with pre-conﬁgured experiments for Ger-\\nman language. Originally, these experiments were\\nsetup to benchmark German BERT. We adopted\\nthis set of hyperparameters without additional grid\\nsearch optimization. FARM is based on Hugging\\nFace’s Transformer library (Wolf et al., 2019). As\\nwe trained GottBERT with fairseq, we converted\\nGottBERT into the Hugging Face format.\\nNamed Entity Recognition We evaluated Got-\\ntBERT on two NER tasks. One was the German\\npart of CoNLL 2003 shared task (Tjong Kim Sang\\nand De Meulder, 2003). It contains three main en-\\ntity classes and one for other miscellaneous entities.\\nAs measurement we used the harmonic mean of\\nprecision and recallF1. The second NER task was\\nGermEval 2014 (Benikova et al., 2014). It extends\\nthe CoNLL 2003 shared task by ﬁne-grained labels\\nand embedded markables. Fine-grained labels al-\\nlow the indication of NER subtypes common in\\nGerman, namely derivations and parts: e.g. “Mann”\\n→ “m¨annlich” and “Mann” → “mannhaft”. In\\norder to recognize nested NEs embedded mark-\\nables are required. Speciﬁcally, this was realized\\n5https://github.com/deepset-ai/FARM\\nby annotating main classes as well as two levels of\\nsubclasses. Performance was measured by the use\\nof an adapted F1 evaluation metric Benikova et al.\\n(2014), which considers the equality of labels and\\nspans (text passages) and additionally levels in the\\nclass hierarchy.\\nText Classiﬁcation GermEval task 2018 (Risch\\net al., 2018) is a text classiﬁcation task that contains\\ntwo subtasks of different granularity: the coarse-\\ngrained binary classiﬁcation of German tweets and\\nﬁne-grained classiﬁcation of the same tweets into\\nfour different classes. Based on the One Million\\nPosts Corpus (Schabus et al., 2017) which is in-\\ntended to test performance on German language,\\nthe 10k German News Articles Dataset (10kG-\\nNAD) topic classiﬁcation6 was created. The dataset\\ncontains approximately 10k news articles of an\\nAustrian newspaper which are to be classiﬁed into\\n9 categories. As both classiﬁcation datasets do not\\nprovide a pre-deﬁned validation set, we used 90%\\nof the original training set for training and 10% for\\nvalidation. Data was split randomly. For evaluation\\nwe computed the mean of the F1-scores of each\\nclass/category.\\n4 Results\\nIn order to evaluate the performance, each down-\\nstream task ran 10 times using different seeds. As\\nmeasure, the F1 scores of the experiments based\\non the test set was used. The score is the best of 10\\nruns of the respective experiment of each trained\\nmodel. The best score selection is based on valida-\\ntion set. We compared GottBERT’s performance\\nwith four other models listed in Table 1.\\nNamed Entity Recognition For the two tested\\nNER tasks, CoNLL 2003 and GermEval 2014, Got-\\ntBERT outperformed all others models followed by\\ndbmz BERT (see Table 2). Obviously, the amount\\nand characteristic of data of the German portion\\nof the OSCAR is beneﬁcial. The third place goes\\nto XLM RoBERTa. Interestingly, mBERT outper-\\nforms German BERT, although mBERT’s amount\\nof German data can be assumed to be smaller com-\\npared to German BERT. Notably, mBERT performs\\nan exponentially smoothed weighting of the data in\\norder to balance the variance in data size of all the\\ndifferent languages. Consequently, even if we knew\\nthe data size of the German portion of mBERT, a\\ndirect comparison would not be possible.\\n6https://tblock.github.io/10kGNAD'), Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2020-12-04T02:10:31+00:00', 'author': '', 'keywords': '', 'moddate': '2020-12-04T02:10:31+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'D:\\\\RAG_Travel_Assistance\\\\rag_travel_assistance\\\\docs\\\\GottbertPaper.pdf', 'total_pages': 6, 'page': 3, 'page_label': '4'}, page_content='Model Type #Languages Data Size Data Source\\nGottBERT RoBERTa 1 145GB OSCAR\\ndbmz BERT BERT 1 16GB\\nWikipedia, EU Bookshop corpus7,\\nOpen Subtitles,\\nCommon-,Para-,NewsCrawl\\nmBERTcased BERT 104 unknown Wikipedia\\nGerman BERT BERT 1 12GB news articles, Open Legal Data8,\\nWikipedia\\nXLM RoBERTa RoBERTa 100 2.5TB\\n(66.6GB German) CommonCrawl, Wikipedia\\nTable 1: This table shows the models, we used in our experiments. Additional information about the pre-training\\nand architecture is listed. Unfortunately, for mBERT we did not ﬁnd any estimate about the data size.\\nModel CoNLL 2003 GermEval 2014\\nGottBERT 83.57 86.84\\ndbmz BERT 82.30 85.82\\nmBERTcased 81.20 85.39\\nGerman BERT 81.18 85.03\\nXLM RoBERTa 81.36 85.41\\nTable 2: F1 scores of NER experiments based on the test set. Best score out of 10 runs (selection based on\\nvalidation set).\\nText Classiﬁcation Table 2 shows the F1 scores\\nof the classiﬁcation tasks. In GermEval 2018\\ncoarse and 10kGNAD, dbmz BERT outperforms\\nthe other models followed by German BERT. Got-\\ntBERT only performs best in GermEval 2018 ﬁne.\\nIn 10kGNAD even XLM RoBERTa outperforms\\nGottBERT. In general, the two RoBERTa based\\nmodels seem not to develop their full potential,\\nwhich might be due to sub-optimal hyperparame-\\nters.\\n5 Conclusion\\nIn this work we present the German single lan-\\nguage RoBERTa based model GottBERT which\\nwas computed on 145GB plain text. GottBERT is\\nthe ﬁrst German single language RoBERTa based\\nmodel. Even without extensive hyperparameter op-\\ntimization, in three out of ﬁve downstream tasks,\\nGottBERT already outperformed all other tested\\nmodels. As the authors of German BERT released\\nthe parameters within the FARM framework, we as-\\nsume the parameters for the downstream tasks are\\nprobably not optimal for RoBERTa based models.\\nConsequently, further extensive hyperparameter op-\\ntimization of the downstream tasks might lead to\\nbetter results for GottBERT. Dodge et al. (2020)\\ngive insights into hyperparameter optimization, its\\ncomplexity and effects in relation to BERT. Fur-\\nther, they present a solution to lower costs. Due\\nto the required effort, we currently leave this open\\nas future work and release GottBERT in hugging-\\nface and fairseq format to the community under the\\nAGPLv3 license.\\nAcknowledgments\\nThis work was supported by the German Min-\\nistry for Education and Research (BMBF FKZ\\n01ZZ1801B) and supported with Cloud TPUs from\\nGoogle’s TensorFlow Research Cloud (TFRC). We\\nwould like to thank Ian Graham for constructive\\ncriticism of the manuscript and Louis Martin for\\nthe helping email contact. A special thanks goes\\nto Myle Ott, who implemented the TPU feature\\nin fairseq and intensively supported us to get our\\ncomputation run. Finally, we would like to recog-\\nnizably thank the people behind the scenes who\\nessentially made this work possible: Frank Werner,\\nGeorg Koch, Friedlinde B¨uhler and Jochen Knaus\\nof our internal IT team, Philipp Munz and Chris-\\ntian Wiedemann of Wabion GmbH and last but not\\nleast Nora Limbourg the Google Cloud Customer\\nEngineer assigned to us.'), Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2020-12-04T02:10:31+00:00', 'author': '', 'keywords': '', 'moddate': '2020-12-04T02:10:31+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'D:\\\\RAG_Travel_Assistance\\\\rag_travel_assistance\\\\docs\\\\GottbertPaper.pdf', 'total_pages': 6, 'page': 4, 'page_label': '5'}, page_content='Model GermEval 2018 coarse GermEval 2018 ﬁne 10kGNAD\\nGottBERT 76.39 51.25 89.20\\ndbmz BERT 77.32 50.97 90.86\\nmBERTcased 72.87 44.78 88.72\\nGerman BERT 77.23 49.28 90.66\\nXLM RoBERTa 75.15 45.63 89.30\\nTable 3: F1 scores of text classiﬁcation experiments based on the test set. Best score out of 10 runs (selection\\nbased on validation set).\\nReferences\\nAlan Akbik, Tanja Bergmann, Duncan Blythe, Kashif\\nRasul, Stefan Schweter, and Roland V ollgraf. 2019.\\nFLAIR: An Easy-to-Use Framework for State-of-\\nthe-Art NLP. In Proceedings of the 2019 Confer-\\nence of the North American Chapter of the Asso-\\nciation for Computational Linguistics (Demonstra-\\ntions), pages 54–59, Minneapolis, Minnesota. Asso-\\nciation for Computational Linguistics.\\nEmily Alsentzer, John R. Murphy, Willie Boag, Wei-\\nHung Weng, Di Jin, Tristan Naumann, and Matthew\\nB. A. McDermott. 2019. Publicly Available Clinical\\nBERT Embeddings. arXiv:1904.03323 [cs]. ArXiv:\\n1904.03323.\\nDarina Benikova, Chris Biemann, Max Kisselew, and\\nSebastian Pad ´o. 2014. GermEval 2014 Named En-\\ntity Recognition Shared Task: Companion Paper.\\nProceedings of the KONVENS GermEval Shared\\nTask on Named Entity Recognition, pages 104–112.\\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and\\nTomas Mikolov. 2017. Enriching Word Vectors with\\nSubword Information. Transactions of the Associa-\\ntion for Computational Linguistics, 5:135–146.\\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\\nVishrav Chaudhary, Guillaume Wenzek, Francisco\\nGuzm´an, Edouard Grave, Myle Ott, Luke Zettle-\\nmoyer, and Veselin Stoyanov. 2019. Unsupervised\\ncross-lingual representation learning at scale. CoRR,\\nabs/1911.02116.\\nPieter Delobelle, Thomas Winters, and Bettina Berendt.\\n2020. RobBERT: a Dutch RoBERTa-based Lan-\\nguage Model. arXiv:2001.06286 [cs]. ArXiv:\\n2001.06286.\\nJacob Devlin. 2018. Multilingual BERT Readme Doc-\\nument.\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\\nKristina Toutanova. 2019. BERT: Pre-training of\\nDeep Bidirectional Transformers for Language Un-\\nderstanding. In Proceedings of the 2019 Conference\\nof the North American Chapter of the Association\\nfor Computational Linguistics: Human Language\\nTechnologies, Volume 1 (Long and Short Papers),\\npages 4171–4186, Minneapolis, Minnesota. Associ-\\nation for Computational Linguistics.\\nJesse Dodge, Gabriel Ilharco, Roy Schwartz, Ali\\nFarhadi, Hannaneh Hajishirzi, and Noah Smith.\\n2020. Fine-Tuning Pretrained Language Models:\\nWeight Initializations, Data Orders, and Early Stop-\\nping. arXiv:2002.06305 [cs]. ArXiv: 2002.06305.\\nYoav Goldberg and Omer Levy. 2014. word2vec Ex-\\nplained: deriving Mikolov et al.’s negative-sampling\\nword-embedding method. arXiv:1402.3722 [cs,\\nstat]. ArXiv: 1402.3722.\\nArmand Joulin, Edouard Grave, Piotr Bojanowski,\\nMatthijs Douze, H ´erve J ´egou, and Tomas Mikolov.\\n2016. FastText.zip: Compressing text classiﬁcation\\nmodels. arXiv preprint arXiv:1612.03651.\\nArmand Joulin, Edouard Grave, Piotr Bojanowski, and\\nTomas Mikolov. 2017. Bag of Tricks for Efﬁcient\\nText Classiﬁcation. In Proceedings of the 15th Con-\\nference of the European Chapter of the Association\\nfor Computational Linguistics: Volume 2, Short Pa-\\npers, pages 427–431. Association for Computational\\nLinguistics.\\nPhilipp Koehn, Hieu Hoang, Alexandra Birch, Chris\\nCallison-Burch, Marcello Federico, Nicola Bertoldi,\\nBrooke Cowan, Wade Shen, Christine Moran,\\nRichard Zens, Chris Dyer, Ondˇrej Bojar, Alexandra\\nConstantin, and Evan Herbst. 2007. Moses: Open\\nSource Toolkit for Statistical Machine Translation.\\nIn Proceedings of the 45th Annual Meeting of the\\nAssociation for Computational Linguistics Compan-\\nion Volume Proceedings of the Demo and Poster Ses-\\nsions, pages 177–180, Prague, Czech Republic. As-\\nsociation for Computational Linguistics.\\nTaku Kudo and John Richardson. 2018. SentencePiece:\\nA simple and language independent subword tok-\\nenizer and detokenizer for Neural Text Processing.\\nIn Proceedings of the 2018 Conference on Empirical\\nMethods in Natural Language Processing: System\\nDemonstrations, pages 66–71, Brussels, Belgium.\\nAssociation for Computational Linguistics.\\nHang Le, Lo¨ıc Vial, Jibril Frej, Vincent Segonne, Max-\\nimin Coavoux, Benjamin Lecouteux, Alexandre Al-\\nlauzen, Benoˆıt Crabb´e, Laurent Besacier, and Didier\\nSchwab. 2020. FlauBERT: Unsupervised Language\\nModel Pre-training for French. arXiv:1912.05372\\n[cs]. ArXiv: 1912.05372.\\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,'), Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2020-12-04T02:10:31+00:00', 'author': '', 'keywords': '', 'moddate': '2020-12-04T02:10:31+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'D:\\\\RAG_Travel_Assistance\\\\rag_travel_assistance\\\\docs\\\\GottbertPaper.pdf', 'total_pages': 6, 'page': 5, 'page_label': '6'}, page_content='Luke Zettlemoyer, and Veselin Stoyanov. 2019.\\nRoBERTa: A Robustly Optimized BERT Pretrain-\\ning Approach. arXiv:1907.11692 [cs]. ArXiv:\\n1907.11692.\\nLouis Martin, Benjamin Muller, Pedro Javier Or-\\ntiz Su ´arez, Yoann Dupont, Laurent Romary, ´Eric\\nde la Clergerie, Djam ´e Seddah, and Beno ˆıt Sagot.\\n2020. CamemBERT: a Tasty French Language\\nModel. In Proceedings of the 58th Annual Meet-\\ning of the Association for Computational Linguistics,\\npages 7203–7219, Online. Association for Computa-\\ntional Linguistics.\\nTomas Mikolov, Edouard Grave, Piotr Bojanowski,\\nChristian Puhrsch, and Armand Joulin. 2018. Ad-\\nvances in Pre-Training Distributed Word Representa-\\ntions. In Proceedings of the Eleventh International\\nConference on Language Resources and Evaluation\\n(LREC 2018), Miyazaki, Japan. European Language\\nResources Association (ELRA).\\nNathan Ng, Kyra Yee, Alexei Baevski, Myle Ott,\\nMichael Auli, and Sergey Edunov. 2019. Facebook\\nFAIR’s WMT19 News Translation Task Submission.\\nIn Proceedings of the Fourth Conference on Ma-\\nchine Translation (Volume 2: Shared Task Papers,\\nDay 1), pages 314–319, Florence, Italy. Association\\nfor Computational Linguistics.\\nPedro Javier Ortiz Su´arez, Laurent Romary, and Benoˆıt\\nSagot. 2020. A Monolingual Approach to Contex-\\ntualized Word Embeddings for Mid-Resource Lan-\\nguages. In Proceedings of the 58th Annual Meet-\\ning of the Association for Computational Linguistics,\\npages 1703–1714, Online. Association for Computa-\\ntional Linguistics.\\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela\\nFan, Sam Gross, Nathan Ng, David Grangier, and\\nMichael Auli. 2019. fairseq: A Fast, Extensible\\nToolkit for Sequence Modeling. arXiv:1904.01038\\n[cs]. ArXiv: 1904.01038.\\nMyle Ott, Sergey Edunov, David Grangier, and\\nMichael Auli. 2018. Scaling Neural Machine Trans-\\nlation. arXiv:1806.00187 [cs]. ArXiv: 1806.00187.\\nJeffrey Pennington, Richard Socher, and Christopher\\nManning. 2014. GloVe: Global Vectors for Word\\nRepresentation. In Proceedings of the 2014 Con-\\nference on Empirical Methods in Natural Language\\nProcessing (EMNLP) , pages 1532–1543, Doha,\\nQatar. Association for Computational Linguistics.\\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\\nDario Amodei, and Ilya Sutskever. 2019. Language\\nmodels are unsupervised multitask learners. OpenAI\\nBlog, 1(8):9.\\nJulian Risch, Eva Krebs, Alexander L ¨oser, Alexander\\nRiese, and Ralf Krestel. 2018. Fine-Grained Classi-\\nﬁcation of Offensive Language. In Proceedings of\\nGermEval 2018 (co-located with KONVENS), pages\\n38–44.\\nDietmar Schabus, Marcin Skowron, and Martin Trapp.\\n2017. One Million Posts: A Data Set of German On-\\nline Discussions. In Proceedings of the 40th Interna-\\ntional ACM SIGIR Conference on Research and De-\\nvelopment in Information Retrieval (SIGIR), pages\\n1241–1244, Tokyo, Japan.\\nMike Schuster and Kaisuke Nakajima. 2012. Japanese\\nand Korean voice search. In 2012 IEEE Interna-\\ntional Conference on Acoustics, Speech and Sig-\\nnal Processing (ICASSP), pages 5149–5152. ISSN:\\n2379-190X.\\nRico Sennrich, Barry Haddow, and Alexandra Birch.\\n2016. Neural Machine Translation of Rare Words\\nwith Subword Units. arXiv:1508.07909 [cs] .\\nArXiv: 1508.07909.\\nErik F. Tjong Kim Sang and Fien De Meulder.\\n2003. Introduction to the CoNLL-2003 Shared Task:\\nLanguage-Independent Named Entity Recognition.\\nIn Proceedings of the Seventh Conference on Nat-\\nural Language Learning at HLT-NAACL 2003 - Vol-\\nume 4, CONLL ’03, pages 142–147, USA. Associ-\\nation for Computational Linguistics. Event-place:\\nEdmonton, Canada.\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\\nKaiser, and Illia Polosukhin. 2017. Attention is All\\nyou Need. In I. Guyon, U. V . Luxburg, S. Bengio,\\nH. Wallach, R. Fergus, S. Vishwanathan, and R. Gar-\\nnett, editors, Advances in Neural Information Pro-\\ncessing Systems 30, pages 5998–6008. Curran Asso-\\nciates, Inc.\\nAntti Virtanen, Jenna Kanerva, Rami Ilo, Jouni Luoma,\\nJuhani Luotolahti, Tapio Salakoski, Filip Ginter, and\\nSampo Pyysalo. 2019. Multilingual is not enough:\\nBERT for Finnish. arXiv:1912.07076 [cs]. ArXiv:\\n1912.07076.\\nWietse de Vries, Andreas van Cranenburgh, Arianna\\nBisazza, Tommaso Caselli, Gertjan van Noord,\\nand Malvina Nissim. 2019. BERTje: A Dutch\\nBERT Model. arXiv:1912.09582 [cs]. ArXiv:\\n1912.09582.\\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\\nChaumond, Clement Delangue, Anthony Moi, Pier-\\nric Cistac, Tim Rault, R ´emi Louf, Morgan Funtow-\\nicz, and Jamie Brew. 2019. Huggingface’s trans-\\nformers: State-of-the-art natural language process-\\ning. CoRR, abs/1910.03771.\\nYang You, Jing Li, Sashank Reddi, Jonathan Hseu,\\nSanjiv Kumar, Srinadh Bhojanapalli, Xiaodan Song,\\nJames Demmel, Kurt Keutzer, and Cho-Jui Hsieh.\\n2020. Large Batch Optimization for Deep Learning:\\nTraining BERT in 76 minutes. arXiv:1904.00962\\n[cs, stat]. ArXiv: 1904.00962 version: 5.'), Document(metadata={'source': 'https://www.seatguru.com/airlines/Lufthansa/baggage.php', 'title': 'Lufthansa: Baggage Fees and Policy - SeatGuru', 'description': ' Before your next Lufthansa flight, be sure to visit our baggage guide to answer some of the most commonly asked questions.', 'language': 'No language found.'}, page_content='Lufthansa: Baggage Fees and Policy - SeatGuru\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSeat Maps\\nAirlines\\nCheap Flights\\n\\nComparison Charts\\n\\n\\n\\nShort-haul Economy Class\\nShort-haul First/Business Class\\nLong-haul Economy Class\\nPremium Economy Class\\nLong-haul Business Class\\nLong-haul First Class\\n\\n\\n\\nRental Cars\\nGuru Tips\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                    Love travel? Sign up for our free newsletter and get the latest news, insights, and money-saving tips.\\n\\t\\t    \\n\\n\\n\\t\\t\\t    By proceeding, you agree to our Privacy Policy and Terms of Use\\n\\n\\n\\n\\n\\n\\n\\n\\nSIGN UP\\n\\n\\nPlease enter a valid email address.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAirlines >Lufthansa >Baggage\\n\\n\\n\\n\\nLufthansa Baggage\\n \\n\\n\\n\\n\\n\\n\\n\\nOverview\\n\\nPlanes & Seat Maps \\nVIEW MORE PLANES  \\n\\nCheck-in\\nBaggage\\nInfants\\nMinors\\nPets\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCarry-On Allowance\\nFirst and Business class passengers are allowed two carry-ons. Economy class and Premium Economy class passengers are permitted one carry-on. Assistive devices and outer garments do not count as personal items. All carry-on luggage must fit in the overhead bin or under the seat in front of you and adhere to the following requirements:\\n\\nMaximum Dimensions of Carry-on Luggage: 55cm x 40cm x 23cm / 22in x 16in x 9in (length x width x height)\\nMaximum Weight of Carry-on Luggage: 8 Kg/18 lb\\n\\nFor more information, please consult Lufthansa\\'s Carry-on baggage information page.\\nREMINDER: Bulkhead seats do not have underseat storage during take-off and landing. Read Bulkheads Explained\\nChecked Baggage Allowance\\nIntercontinental flights\\nThe following basic regulations apply for the free baggage allowance on all intercontinental flights operated by Lufthansa:\\n\\nEconomy Class: 1 bag up to 23 kg (50 lb)\\nPremium Economy Class: 2 bags up to 23 kg (50 lb) each\\nBusiness Class: 2 bags up to 32 kg (70 lb) each\\nFirst Class: 3 bags up to 32 kg (70 lb) each\\n\\nFree baggage allowance for status customers on intercontinental flights\\nIf you are traveling on Lufthansa\\'s \"Economy Light\" product,\\xa0you are allowed one piece of carry-on baggage. This fare includes no free baggage allowance, however, a piece of checked baggage can be added to your booking for a fee.\\nFrequent Traveller\\n\\nEconomy Class: 2 bags up to 23 kg (50 lb)\\nPremium Economy Class: 2 bags up to 23 kg (50 lb) each\\nBusiness Class: 2 bags up to 32 kg (70 lb) each\\nFirst Class: 3 bags up to 32 kg (70 lb) each\\n\\nSenator / HON Circle Member / Star Alliance Gold member\\n\\nEconomy Class: 2 bags up to 23 kg (50 lb)\\nPremium Economy Class: 3 bags up to 23 kg (50 lb) each\\nBusiness Class: 3 bags up to 32 kg (70 lb) each\\nFirst Class: 4 bags up to 32 kg (70 lb) each\\nAdditional sports bag: + 1 golf bag\\n\\nEuropean flights\\nFor flights within Europe the free baggage rules are determined by the new Europe fare options:\\n\\nEconomy Light: 1 piece of carry-on baggage up to 8 kg only\\nEconomy Classic / Flex: 1 piece of carry-on baggage up to 8 kg; 1 bag up to 23 kg\\nBusiness Class: 2 pieces of carry-on baggage up to 8 kg; 2 bags up 32 kg\\n\\nFree baggage allowance for status customers on intercontinental flights\\nFrequent Traveller\\n\\nEconomy Class Light: Carry-on baggage only\\nEconomy Class Classic: 2 bags up to 23 kg (50 lb)\\nEconomy Class Flex: Flex: 2 bags up to 23 kg (50 lb)\\nBusiness Class: 2 bags up to 32 kg. (70 lb)\\n\\nSenator / HON Circle Member / Star Alliance Gold member\\n\\nEconomy Class Light: Carry-on baggage only\\nEconomy Class Classic: 2 bags up to 23 kg (50 lb)\\nEconomy Class Flex: Flex: 2 bags up to 23 kg (50 lb)\\nBusiness Class: 2 bags up to 32 kg. (70 lb)\\nAdditional sports bag: + 1 golf bag'), Document(metadata={'source': 'https://www.seatguru.com/airlines/Lufthansa/baggage.php', 'title': 'Lufthansa: Baggage Fees and Policy - SeatGuru', 'description': ' Before your next Lufthansa flight, be sure to visit our baggage guide to answer some of the most commonly asked questions.', 'language': 'No language found.'}, page_content=\"European flights\\nFor flights within Europe the free baggage rules are determined by the new Europe fare options:\\n\\nEconomy Light: 1 piece of carry-on baggage up to 8 kg only\\nEconomy Classic / Flex: 1 piece of carry-on baggage up to 8 kg; 1 bag up to 23 kg\\nBusiness Class: 2 pieces of carry-on baggage up to 8 kg; 2 bags up 32 kg\\n\\nFree baggage allowance for status customers on intercontinental flights\\nFrequent Traveller\\n\\nEconomy Class Light: Carry-on baggage only\\nEconomy Class Classic: 2 bags up to 23 kg (50 lb)\\nEconomy Class Flex: Flex: 2 bags up to 23 kg (50 lb)\\nBusiness Class: 2 bags up to 32 kg. (70 lb)\\n\\nSenator / HON Circle Member / Star Alliance Gold member\\n\\nEconomy Class Light: Carry-on baggage only\\nEconomy Class Classic: 2 bags up to 23 kg (50 lb)\\nEconomy Class Flex: Flex: 2 bags up to 23 kg (50 lb)\\nBusiness Class: 2 bags up to 32 kg. (70 lb)\\nAdditional sports bag: + 1 golf bag\\n\\nFree baggage dimensions\\nThe maximum size per piece of baggage, regardless of class, is 158 cm (width + height + depth). Items of baggage that are larger or heavier than the permitted dimensions and weight or that are additional to the free baggage allowance will be carried as excess baggage for a flat fee. Items that weigh more than 32 kg will be transported by air freight for a charge.\\nFor more information on baggage allowances for specific routes, visit  Lufthansa's baggage site.\\nAdditionally, Lufthansa allows for the free transport of a pushchair or baby basket. Disabled passengers are allowed free transport of two wheelchairs or other mobility aids in addition to their free baggage allowance.\\nOverweight Baggage Fees\\nHeavier than free baggage allowance (24 – 32 kg): 50 € / USD 70 within Europe / between third countries and 100 € / USD 150 on intercontinental flights\\nExcess and Oversize Baggage Fees\\nEconomy and Premium Economy Class\\nLarger than free baggage allowance (from 159 cm): 100 € / USD 150 within Europe / between third countries and 200 € / USD 300 on intercontinental flights\\nLarger and heavier than free baggage allowance (24 – 32 kg and from 159 cm): 150 € / USD 220 within Europe / between third countries and 300 € / USD 450 on intercontinental flights.\\nBusiness and First Class\\nLarger than free baggage allowance (from 159 cm): 100 € / USD 150 within Europe / between third countries and 200 € / USD 300 on intercontinental flights.\\nSports Equipment\\nSports baggage is carried free of charge as part of your permitted free baggage allowance.\\nSports baggage must be registered up to 24 hours before departure via the reservation hotline.\\nIn addition to your free baggage allowance, you may take one piece of ski or snowboarding equipment free of charge on your Lufthansa flight. The maximum weight of an additional piece of ski or snowboarding equipment must be within the weight limits of the travel class you have booked. Please note that this rule only applies to flights operated by Lufthansa.\\nBecause of limited capacities and differing rules on partner airlines, Lufthansa cannot guarantee when you book that your ski baggage will be accepted in every case. Please register your ski bag up to 24 hours before departure via the reservation hotline, regardless of whether your ski bag is within your free baggage allowance or is being checked in as excess baggage.\\nCPAP Devices:\\nLufthansa will accept assistive devices with batteries as checked baggage as well as inside the cabin. Assistive devices with batteries include CPAP machines. Lufthansa will allow qualified individuals with a disability who are using FAA approved personal respirators/ventilators to bring their equipment, including non-spillable batteries, onboard the aircraft. You must have sufficient battery power for 150% of the maximum flight duration.\\n\\xa0\\n\\n\\n\\n \\n\\n\\n \\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSeatGuru was created to help travelers choose the best seats and in-flight amenities. \\n\\n\\n\\n\\nForum\\n\\nMobile\\n\\nFAQContact UsSite Map \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCopyright © TripAdvisor LLC, 2001 - 2025. All rights reserved.\\n\\nPrivacy and Cookie Statement\\nTerms and Conditions\")]\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=5000, chunk_overlap=1000)\n",
    "document_chunks = text_splitter.split_documents(docs)\n",
    "print(document_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_db = Chroma.from_documents(document_chunks, OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve\n",
    "\n",
    "def _get_context_retriever_chain(vector_db, llm):\n",
    "    retriever = vector_db.as_retriever()\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        (\"user\", \"{input}\"),\n",
    "        (\"user\", \"Given the above conversation, generate a search query to look up in order to get inforamtion relevant to the conversation, focusing on the most recent messages.\"),\n",
    "    ])\n",
    "    retriever_chain = create_history_aware_retriever(llm, retriever, prompt)\n",
    "\n",
    "    return retriever_chain"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_travel_assistance",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
